{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# Add ../src to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('__file__'), '../src'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import the parse config function to parse the .toml file\n",
    "from utils.config_tool import parse_config\n",
    "from engine.clean import clean_All_news as cc\n",
    "\n",
    "def notify(title, text):\n",
    "    os.system(\"\"\"\n",
    "              osascript -e 'display notification \"{}\" with title \"{}\"'\n",
    "              \"\"\".format(text, title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config_file = \"../config/predict_stock_w_news.toml\"\n",
    "config = parse_config(config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The load path and save path\n",
    "load_path = os.path.join(config['info']['local_data_path'],'data_raw', config['news_ingestion']['input'][\"news_head_line\"])\n",
    "save_path = os.path.join(config['info']['local_data_path'],'data_clean', config['news_preprocessing']['output'][\"news_head_line_cleaned\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the raw data\n",
    "df = pd.read_json(load_path, lines=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicated rows which is not null\n",
    "cleaned = pd.concat([df[(df['headline'] != '') & (df['short_description'] != '')].drop_duplicates(subset=['headline', 'short_description']), df[(df['headline'] == '') | (df['short_description'] == '')]])\n",
    "\n",
    "# Combine the headline and the summary\n",
    "cleaned['headline_summary'] = cleaned['headline'] + ' ' + cleaned['short_description']\n",
    "\n",
    "# Drop the original headline and summary and the columns that are not needed\n",
    "cleaned.drop(columns=['headline', 'short_description', 'authors', 'link', 'date'], inplace=True)\n",
    "\n",
    "# Drop the rows with missing values\n",
    "headline = cleaned[cleaned['headline_summary'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline[headline['headline_summary'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicated rows\n",
    "\n",
    "headline.drop_duplicates(subset=['headline_summary'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline[headline['headline_summary'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the tokenized headline and summary\n",
    "headline['headline_summary_tokenized'] = headline['headline_summary'].apply(cc.get_tokenized_words_with_no_punctuation)\n",
    "# remove the stop words and lemmatize the words\n",
    "headline['headline_summary_tokenized'] = headline['headline_summary_tokenized'].apply(cc.remove_stop_words)\n",
    "headline['headline_summary_tokenized'] = headline['headline_summary_tokenized'].apply(cc.lemmatize_words)\n",
    "# Drop the orginal headline and summary\n",
    "headline.drop(columns=['headline_summary'], inplace=True)\n",
    "# NER\n",
    "headline = pd.concat([headline, headline['headline_summary_tokenized'].apply(lambda x : cc.extract_ner_features(''.join(x)))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with less than 10 words\n",
    "headline = headline[headline['headline_summary_tokenized'].apply(lambda x : len(x)) >= 10]\n",
    "\n",
    "# Save the cleaned data\n",
    "headline.to_csv(save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline_summary_tokenized</th>\n",
       "      <th>PERSON</th>\n",
       "      <th>ORG</th>\n",
       "      <th>GPE</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>PRODUCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>['4', 'million', 'american', 'roll', 'sleeve',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>['american', 'airline', 'flyer', 'charged', 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>['23', 'funniest', 'tweet', 'cat', 'dog', 'wee...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>['funniest', 'tweet', 'parent', 'week', 'accid...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>['woman', 'called', 'cop', 'black', 'loses', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                         headline_summary_tokenized  PERSON  ORG  \\\n",
       "0  U.S. NEWS  ['4', 'million', 'american', 'roll', 'sleeve',...       0    0   \n",
       "1  U.S. NEWS  ['american', 'airline', 'flyer', 'charged', 'b...       0    0   \n",
       "2     COMEDY  ['23', 'funniest', 'tweet', 'cat', 'dog', 'wee...       0    0   \n",
       "3  PARENTING  ['funniest', 'tweet', 'parent', 'week', 'accid...       0    0   \n",
       "4  U.S. NEWS  ['woman', 'called', 'cop', 'black', 'loses', '...       0    0   \n",
       "\n",
       "   GPE  EVENT  PRODUCT  \n",
       "0    0      0        0  \n",
       "1    0      0        0  \n",
       "2    0      0        0  \n",
       "3    0      0        0  \n",
       "4    0      0        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the cleaned data\n",
    "\n",
    "headline = pd.read_csv(save_path)\n",
    "headline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "count    178064.000000\n",
      "mean        185.469281\n",
      "std          63.379008\n",
      "min          72.000000\n",
      "25%         142.000000\n",
      "50%         177.000000\n",
      "75%         216.000000\n",
      "max        1402.000000\n",
      "Name: headline_summary_tokenized, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop = headline['headline_summary_tokenized'].apply(lambda tokens: tokens in stop_words)\n",
    "print(stop.astype(int).sum())\n",
    "null = headline['headline_summary_tokenized'].apply(lambda tokens: tokens == [])\n",
    "print(null.astype(int).sum())\n",
    "lenth = headline['headline_summary_tokenized'].apply(lambda tokens: len(tokens))\n",
    "print(lenth.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data:\n",
    "\n",
    "Since the the category is not evenly distributed, we need to find a training set which samples 10000 elements from each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the category we want to predict\n",
    "categorys_to_predict = ['POLITICS', 'WELLNESS', 'ENTERTAINMENT', 'ELSE']\n",
    "\n",
    "# Rename the category\n",
    "headline['category'] = headline['category'].apply(lambda x: x if x in categorys_to_predict else 'ELSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ELSE             116515\n",
       "POLITICS          30775\n",
       "WELLNESS          17854\n",
       "ENTERTAINMENT     12920\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/3779012536.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled = headline.groupby('category').apply(lambda x: x.sample(sameple_size, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Sampling the data\n",
    "sameple_size = 10000\n",
    "sampled = headline.groupby('category').apply(lambda x: x.sample(sameple_size, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ELSE             10000\n",
       "ENTERTAINMENT    10000\n",
       "POLITICS         10000\n",
       "WELLNESS         10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Choose the words that appear in at least 5 documents and at most 50% of the documents\n",
    "tfidf = TfidfVectorizer(max_df=0.5, min_df=5)\n",
    "tfidf_matrix = tfidf.fit_transform(sampled['headline_summary_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After I fit the tfidf, I can transform the new data\n",
    "diff = 0\n",
    "\n",
    "for i in range(len(sampled)):\n",
    "    a1 = tfidf.transform([sampled['headline_summary_tokenized'][i]]).toarray()\n",
    "    a2 = tfidf_matrix[i].toarray()\n",
    "    # If the error is larger than 1e-15, print the index, otherwise, the error is due to the floating point precision\n",
    "    if np.abs(a1-a2).max() >= 1e-15:\n",
    "        print(i)\n",
    "        diff += np.abs(a1-a2).sum()\n",
    "\n",
    "\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tfidf\n",
    "\n",
    "\n",
    "tfidf_path = os.path.join(config['info']['local_data_path'], 'model_news', config['news_model']['input']['tfidf_headline'])\n",
    "\n",
    "\n",
    "joblib.dump(tfidf, tfidf_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best practice: Use `tfidf_matrix` to maintain consistency with original transformation.\n",
    "\n",
    "It records the original data information. For new data, we simply use `tfidf.transform`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonnegative matrix factorization\n",
    "\n",
    "When consider the Tfidf matrix, each coefficient is nonnegative. And when the number of tokens are large, the matrix is very sparse. So instead of using PCA, it is better to consider the nonnegative matrix factorization (NMF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best number of components\n",
    "#! Warning: It takes a long time to run\n",
    "\n",
    "start = time.time()\n",
    "n_components = [100, 200, 300, 400, 500]\n",
    "rss = []\n",
    "t0 = start\n",
    "for n in n_components:\n",
    "    nmf = NMF(n_components=n, max_iter=500)\n",
    "    E_matrix = nmf.fit_transform(tfidf_matrix)\n",
    "    print(f\"To process {n} components, it takes {time.time()-t0} seconds\")\n",
    "    t0 = time.time()\n",
    "    rss.append(nmf.reconstruction_err_)\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute Residual Sum of Squares (RSS) for Non-Negative Matrix Factorization (NMF)\n",
    "# rss = np.sum(np.square(tfidf_matrix - E_matrix @ nmf.components_))\n",
    "# rss1 = nmf.reconstruction_err_\n",
    "# print(rss, rss1**2)\n",
    "\n",
    "print(f\"The rss is {rss}\")\n",
    "print(f\"The best number of components is {n_components[np.argmin(rss)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best number of compoenets. Note that the time I run:\n",
    "``` python\n",
    "import time\n",
    "start = time.time()\n",
    "n_components = [100, 200, 300, 400, 500]\n",
    "rss = []\n",
    "t0 = start\n",
    "for n in n_components:\n",
    "    nmf = NMF(n_components=n, max_iter=500)\n",
    "    E_matrix = nmf.fit_transform(tfidf_matrix)\n",
    "    print(f\"To process {n} components, it takes {time.time()-t0} seconds\")\n",
    "    t0 = time.time()\n",
    "    rss.append(nmf.reconstruction_err_)\n",
    "```\n",
    "\n",
    "**The output:**\n",
    "``` zsh\n",
    "To process 100 components, it takes 52.489633083343506 seconds\n",
    "To process 200 components, it takes 256.3671808242798 seconds\n",
    "To process 300 components, it takes 2259.08752989769 seconds\n",
    "To process 400 components, it takes 6990.929425239563 seconds\n",
    "To process 500 components, it takes 7964.212979078293 seconds\n",
    "```\n",
    "\n",
    "And the residual sum of squares (reconstruction error):\n",
    "``` python\n",
    "print(f\"The rss is {rss}\")\n",
    "print(f\"The best number of components is {n_components[np.argmin(rss)]}\")\n",
    "```\n",
    "\n",
    "**The output:**\n",
    "``` zsh\n",
    "The rss is [np.float64(188.4237102845113), np.float64(181.7639160482998), np.float64(176.32832738298404), np.float64(171.49108989213317), np.float64(167.21638211672698)]\n",
    "The best number of components is 500\n",
    "```\n",
    "The whole procedure will take ~ 10 hours, so we do not want to run this again.\n",
    "\n",
    "It turns out that the more components we want to get, the longer time will be needed. Of course it will decrease the error, but not significantly. To balance this, keep 500 componenets will need ~ 2 hours, which is acceptable. So it is better to work on 100 and 200 componenets first, and if it performs bad, we may need to increase the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets keep 200 compon_components = 200\n",
    "\n",
    "nmf = NMF(n_components=200, max_iter=500)\n",
    "E_matrix = nmf.fit_transform(tfidf_matrix)\n",
    "error = nmf.reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "nmf_model_path = os.path.join(config['info']['local_data_path'], 'model_news', config['news_model']['input']['tfidf_nmf_headline'])\n",
    "\n",
    "joblib.dump(nmf, nmf_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model\n",
    "tfidf1 = joblib.load(tfidf_path)\n",
    "nmf1 = joblib.load(nmf_model_path)\n",
    "print(error, nmf1.reconstruction_err_)\n",
    "# Get the E matrix\n",
    "E_matrix1 = nmf1.transform(tfidf1.transform(sampled['headline_summary_tokenized']))\n",
    "print(np.abs(E_matrix-E_matrix1).max())\n",
    "print(E_matrix1.shape, E_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small difference (0.00086) between E_matrix and E_matrix1 occurs because:\n",
    "\n",
    "- Floating point precision limitations\n",
    "- NMF transformation is not exactly deterministic\n",
    "- Multiple matrix operations compound small differences\n",
    "\n",
    "The matrices are effectively identical since:\n",
    "\n",
    "- Max difference is ~0.00086 (less than 0.1%)\n",
    "- Shapes match exactly (40000, 200)\n",
    "- Reconstruction errors are identical (181.79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Word2Vec embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to define the class for the word2vec\n",
    "# Word2Vec doesn't have fit_transform like sklearn, so we need to define the class to fit_transform\n",
    "\n",
    "class Word2VecVectorizer:\n",
    "    \"\"\" We need to define the class for the word2vec so that we can do fit_transform like sklearn\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\" Word2VecVectorized does not support workers < 1, so we set the default workers to 4 \"\"\"\n",
    "    def __init__(self, vector_size=200, window=5, min_count=1, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.word2vec = None\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.word2vec = Word2Vec(X, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        return np.vstack([np.mean([self.word2vec.wv[word] for word in doc if word in self.word2vec.wv], axis=0) for doc in X])\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.word2vec is None:\n",
    "            raise ValueError(\"The model is not fitted\")\n",
    "        return np.vstack([np.mean([self.word2vec.wv[word] for word in doc if word in self.word2vec.wv], axis=0) for doc in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the word2vec model\n",
    "\n",
    "word2vec = Word2VecVectorizer()\n",
    "word2vec_matrix = word2vec.fit_transform(sampled['headline_summary_tokenized'])\n",
    "print(word2vec_matrix.shape)\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"The preprocessing is done\", \"The preprocessing is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word2vec model\n",
    "\n",
    "word2vec_path = os.path.join(config['info']['local_data_path'], 'model_news', config['news_model']['input']['word2vec_headline'])\n",
    "\n",
    "joblib.dump(word2vec, word2vec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics to measure the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    lables = ['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS']\n",
    "    confusion = confusion_matrix(y_true, y_pred,labels = lables)\n",
    "    print(f\"The accuracy is {accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"The confusion matrix \\n{lables} is \\n{confusion}\")\n",
    "\n",
    "    # Map 'ELSE' to 0 (negative) and others to 1 (positive)\n",
    "    y_test_binary = [0 if label == 'ELSE' else 1 for label in y_true]\n",
    "    y_pred_binary = [0 if label == 'ELSE' else 1 for label in y_pred]\n",
    "\n",
    "    # Compute the F1 score (treating 1 as the positive class)\n",
    "    f1 = f1_score(y_test_binary, y_pred_binary, pos_label=1, average='binary')\n",
    "    print(f\"F1 Score when we treat ELSE as negative: {f1}\")\n",
    "    f1_seperate = f1_score(y_true, y_pred, pos_label=1, average=None)\n",
    "    print(f\"F1 Score for everclass: \\n{lables} \\n{f1_seperate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misclassified_samples(y_test, y_pred):\n",
    "    misclassified_idx = np.where(y_test != y_pred)[0]\n",
    "    sampled_test = sampled.loc[y_test.index]\n",
    "    results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialte a dataframe to store the misclassified index of each model\n",
    "misclassified = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train test split and kfold\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-saved features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = os.path.join(config['info']['local_data_path'], 'model_news', config['news_model']['input']['tfidf_headline'])\n",
    "nmf_model_path = os.path.join(config['info']['local_data_path'], 'model_news', config['news_model']['input']['tfidf_nmf_headline'])\n",
    "\n",
    "\n",
    "# Load the features using joblib\n",
    "\n",
    "tfidf = joblib.load(tfidf_path)\n",
    "nmf = joblib.load(nmf_model_path)\n",
    "#word2vec = joblib.load(word2vec_path)\n",
    "\n",
    "# Transform the data\n",
    "transformed_data = nmf.transform(tfidf.transform(sampled['headline_summary_tokenized']))\n",
    "\n",
    "# Transform the data using word2vec\n",
    "#transformed_data = word2vec.transform(sampled['headline_summary_tokenized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the standard machine learning algorithms, we choose Tf-idf embedding. It will out perform the word2vec embedding a lot. \n",
    "\n",
    "Later, when we try to do the neural network, we can try word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_data, sampled['category'], test_size=0.2, stratify=sampled['category'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the knn model\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, choose the best number of neighbors using cross validation\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, random_state =42, shuffle=True)\n",
    "\n",
    "i=0\n",
    "ks = range(1, 1000)\n",
    "accs=np.zeros((5, len(ks)))\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    X_tt, X_ho = X_train[train_index], X_train[test_index]\n",
    "    y_tt, y_ho = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    for k in ks:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_tt, y_tt)\n",
    "        accs[i, k-1] = accuracy_score(y_ho, knn.predict(X_ho))\n",
    "    i += 1\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"KNN\", \"KNN has finished training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best number of neighbors\n",
    "accs_mean = accs.mean(axis=0)\n",
    "best_k = ks[np.argmax(accs_mean)]\n",
    "\n",
    "print(f\"The best number of neighbors is {best_k}\")\n",
    "notify(\"KNN\", f\"The best number of neighbors is {best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal k is ~70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(ks, accs_mean)\n",
    "\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN: Accuracy vs Number of Neighbors')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the grid search to find the best parameters for the KNN model\n",
    "\n",
    "param_grid = {'n_neighbors': range(1, 1000)}\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best parameters are {grid_search.best_params_}\")\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"KNN\", f\"The best parameters are {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are {'n_neighbors': 38}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.5405\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[ 977  588  161  274]\n",
      " [ 447 1246  147  160]\n",
      " [ 466  302 1099  133]\n",
      " [ 569  377   52 1002]]\n",
      "F1 Score when we treat ELSE as negative: 0.7829477514946712\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.43821485 0.55218258 0.63544377 0.56150182]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "# The best parameter:\n",
    "\n",
    "#best_k = grid_search.best_params_['n_neighbors']\n",
    "best_k=38\n",
    "\n",
    "knn_best = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_best.fit(X_train, y_train)\n",
    "y_pred = knn_best.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)\n",
    "mis_KNN = get_misclassified_samples(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_KNN.rename(columns={'index': 'index_KNN'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_KNN['index_KNN']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import svm model\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, find the best hyperparameters using cross validation 1:\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, random_state =42, shuffle=True)\n",
    "Cs = [0.1, 1, 10, 100, 1000]\n",
    "#kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "degrees = range(1, 10)\n",
    "\n",
    "i=0\n",
    "start = time.time()\n",
    "t0 = time.time()\n",
    "accs_svc_poly = np.zeros((5, len(Cs), len(degrees)))\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    X_tt, X_ho = X_train[train_index], X_train[test_index]\n",
    "    y_tt, y_ho = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    for c in Cs:\n",
    "        for d in degrees:\n",
    "            print(f\"start to process {c} and {d} in the {i+1}th fold\")\n",
    "            svc = SVC(C=c, kernel='poly', degree=d)\n",
    "            svc.fit(X_tt, y_tt)\n",
    "            accs_svc_poly[i, Cs.index(c), degrees.index(d)] = accuracy_score(y_ho, svc.predict(X_ho))\n",
    "            print(f\"To process {c} and {d} in the {i+1}th fold, it takes {time.time()-t0} seconds\")\n",
    "            t0 = time.time()\n",
    "    i += 1\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"SVM\", \"SVM has finished training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_svc_poly_mean = accs_svc_poly.mean(axis=0)\n",
    "best_c, best_d = np.unravel_index(np.argmax(accs_svc_poly_mean), accs_svc_poly_mean.shape)\n",
    "print(f\"The best C is {Cs[best_c]} and the best degree is {degrees[best_d]}\")\n",
    "notify(\"SVM\", f\"The best C is {Cs[best_c]} and the best degree is {degrees[best_d]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(Cs, accs_svc_poly_mean[:, best_d])\n",
    "\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('SVM: Accuracy vs C')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like when C is getting larger, the accuracy is getting better. A good trade-off is C=100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(degrees, accs_svc_poly_mean[best_c])\n",
    "\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('SVM: Accuracy vs Degree')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like when the degree of the polynomial is 1 is the optimal. The accuracy is monotonically decreasing. The procedure takes ~6 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy\n",
    "svc = SVC(C=Cs[best_c], kernel='poly', degree=degrees[best_d])\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output:\n",
    "``` bash\n",
    "The accuracy is 0.7225\n",
    "The confusion matrix is \n",
    "[[1232  305  196  314]\n",
    " [ 398 1382  121   66]\n",
    " [ 291   65 1585   45]\n",
    " [ 349   47   23 1581]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best kernel and C using cross validation:\n",
    "kfold = StratifiedKFold(n_splits=5, random_state =42, shuffle=True)\n",
    "Cs = [0.1, 1, 10, 100, 1000]\n",
    "kernels = ['linear', 'rbf', 'sigmoid']\n",
    "\n",
    "i=0\n",
    "start = time.time()\n",
    "t0 = time.time()\n",
    "accs_svc = np.zeros((5, len(Cs), len(kernels)))\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    X_tt, X_ho = X_train[train_index], X_train[test_index]\n",
    "    y_tt, y_ho = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    for c in Cs:\n",
    "        for kernel in kernels:\n",
    "            print(f\"start to process {c} and {kernel} in the {i+1}th fold\")\n",
    "            svc = SVC(C=c, kernel=kernel)\n",
    "            svc.fit(X_tt, y_tt)\n",
    "            accs_svc[i, Cs.index(c), kernels.index(kernel)] = accuracy_score(y_ho, svc.predict(X_ho))\n",
    "            print(f\"To process {c} and {kernel} in the {i+1}th fold, it takes {time.time()-t0} seconds\")\n",
    "            t0 = time.time()\n",
    "    i += 1\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"SVM\", \"SVM has finished training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_svc_mean = accs_svc.mean(axis=0)\n",
    "best_c, best_kernel = np.unravel_index(np.argmax(accs_svc_mean), accs_svc_mean.shape)\n",
    "print(accs_svc_mean)\n",
    "print(f\"The best C is {Cs[best_c]} and the best kernel is {kernels[best_kernel]}\")\n",
    "notify(\"SVM\", f\"The best C is {Cs[best_c]} and the best kernel is {kernels[best_kernel]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best C is 10 and the best kernel is rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(Cs, accs_svc_mean[:, best_kernel])\n",
    "\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('SVM: Accuracy vs C')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy\n",
    "svc = SVC(C=Cs[best_c], kernel=kernels[best_kernel])\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the grid search to find the best hyperparameters for the SVM model\n",
    "\n",
    "param_grid_1 = {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'rbf', 'sigmoid']}\n",
    "param_grid_2 = {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['poly'], 'degree': range(1, 10)}\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "grid_search_1 = GridSearchCV(svc, param_grid_1, cv=5, n_jobs=-1)\n",
    "grid_search_1.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best parameters are {grid_search_1.best_params_}\")\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"SVM\", f\"The best parameters are {grid_search_1.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```zsh\n",
    "The best parameters are {'C': 1000, 'kernel': 'linear'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output\n",
    "svc_bestkernel = SVC(**grid_search_1.best_params_)\n",
    "svc_bestkernel.fit(X_train, y_train)\n",
    "y_pred = svc_bestkernel.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "``` zsh\n",
    "The accuracy is 0.70325\n",
    "The confusion matrix \n",
    "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
    "[[1266  277  194  310]\n",
    " [ 446 1307  145   69]\n",
    " [ 335   79 1519   53]\n",
    " [ 387   45   34 1534]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "grid_search_2 = GridSearchCV(svc, param_grid_2, cv=5, n_jobs=-1)\n",
    "\n",
    "grid_search_2.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best parameters are {grid_search_2.best_params_}\")\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"SVM\", f\"The best parameters are {grid_search_2.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "``` zsh\n",
    "The best parameters are {'C': 1000, 'degree': 1, 'kernel': 'poly'}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.695\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[1190  328  194  288]\n",
      " [ 425 1340  132  103]\n",
      " [ 351  100 1505   44]\n",
      " [ 368   77   30 1525]]\n",
      "F1 Score when we treat ELSE as negative: 0.8325047145551174\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.54914629 0.6970091  0.77959078 0.77020202]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "# the output\n",
    "\n",
    "#svc_bestdegree = SVC(**grid_search_2.best_params_)\n",
    "svc_bestdegree = SVC(C=1, kernel='poly', degree=1)\n",
    "\n",
    "#Fit the model:\n",
    "svc_bestdegree.fit(X_train, y_train)\n",
    "y_pred = svc_bestdegree.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)\n",
    "mis_SVM = get_misclassified_samples(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "``` zsh\n",
    "The accuracy is 0.72175\n",
    "The confusion matrix \n",
    "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
    "[[1246  298  192  311]\n",
    " [ 399 1385  122   61]\n",
    " [ 293   79 1570   44]\n",
    " [ 347   50   30 1573]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_SVM.rename(columns={'index': 'index_SVM'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_SVM['index_SVM']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, find the best hyperparameters using cross validation 1:\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, random_state =42, shuffle=True)\n",
    "Cs = [0.1, 1, 10, 100, 1000]\n",
    "solver = ['newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "accs = np.zeros((5, len(Cs), len(solver)))\n",
    "\n",
    "i=0\n",
    "start = time.time()\n",
    "t0 = time.time()\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    X_tt, X_ho = X_train[train_index], X_train[test_index]\n",
    "    y_tt, y_ho = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    for c in Cs:\n",
    "        for s in solver:\n",
    "            print(f\"start to process {c} and {s} in the {i+1}th fold\")\n",
    "            lr = LogisticRegression(C=c, solver=s)\n",
    "            lr.fit(X_tt, y_tt)\n",
    "            accs[i, Cs.index(c), solver.index(s)] = accuracy_score(y_ho, lr.predict(X_ho))\n",
    "            print(f\"To process {c} and {s} in the {i+1}th fold, it takes {time.time()-t0} seconds\")\n",
    "            t0 = time.time()\n",
    "    i += 1\n",
    "\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"Logistic\", \"Logistic has finished training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_mean = accs.mean(axis=0)\n",
    "print(accs_mean)\n",
    "best_c, best_s = np.unravel_index(np.argmax(accs_mean), accs_mean.shape)\n",
    "print(f\"The best C is {Cs[best_c]} and the best solver is {solver[best_s]}\")\n",
    "notify(\"SVM\", f\"The best C is {Cs[best_c]} and the best solver is {solver[best_s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best C is 1000 and the best solver is newton-cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(Cs, accs_mean[:, best_s])\n",
    "\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Logistic Regression: Accuracy vs C')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy\n",
    "lr = LogisticRegression(C=Cs[best_c], solver=solver[best_s])\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output:\n",
    "``` zsh\n",
    "The accuracy is 0.720875\n",
    "The confusion matrix is \n",
    "[[1127  350  224  346]\n",
    " [ 314 1440  135   78]\n",
    " [ 248   91 1592   55]\n",
    " [ 292   68   32 1608]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the grid search to find the best hyperparameters for the logistic regression model\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], 'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']}\n",
    "lr = LogisticRegression()\n",
    "\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best parameters are {grid_search.best_params_}\")\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"Logistic\", f\"The best parameters are {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.708875\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[1075  379  224  322]\n",
      " [ 321 1444  130  105]\n",
      " [ 270  119 1552   59]\n",
      " [ 296   75   29 1600]]\n",
      "F1 Score when we treat ELSE as negative: 0.8494766572520352\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.54265522 0.71894449 0.7888183  0.78316202]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "# the output\n",
    "\n",
    "#lr_best = LogisticRegression(**grid_search.best_params_)\n",
    "lr_best = LogisticRegression(C=1000, solver='newton-cg')\n",
    "\n",
    "lr_best.fit(X_train, y_train)\n",
    "y_pred = lr_best.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)\n",
    "mis_log = get_misclassified_samples(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_log.rename(columns={'index': 'index_log'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_log['index_log']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Naive Baysian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "kfold = StratifiedKFold(n_splits=5, random_state =42, shuffle=True)\n",
    "\n",
    "accs = np.zeros((5, 2))\n",
    "models = [GaussianNB(), MultinomialNB()]\n",
    "i = 0\n",
    "accs = np.zeros((5, 2))\n",
    "start = time.time()\n",
    "t0 = start\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    X_tt, X_ho = X_train[train_index], X_train[test_index]\n",
    "    y_tt, y_ho = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(f\"start to process {model} in the {i+1}th fold\")\n",
    "        model.fit(X_tt, y_tt)\n",
    "        accs[i, j] = accuracy_score(y_ho, model.predict(X_ho))\n",
    "        print(f\"To process {model} in the {i+1}th fold, it takes {time.time()-t0} seconds\")\n",
    "        t0 = time.time()\n",
    "    i += 1\n",
    "\n",
    "notify(\"Naive Bayes\", \"Naive Bayes has finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accs = accs.mean(axis=0)\n",
    "print(mean_accs)\n",
    "best_model = np.argmax(mean_accs)\n",
    "print(f\"The best model is {models[best_model]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.64625\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[ 878  340  274  508]\n",
      " [ 331 1221  197  251]\n",
      " [ 243  123 1477  157]\n",
      " [ 243  109   54 1594]]\n",
      "F1 Score when we treat ELSE as negative: 0.8424217797643234\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.47523681 0.64381756 0.73813093 0.70687361]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "# get accuracy\n",
    "\n",
    "#model = models[best_model]\n",
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)\n",
    "mis_NB = get_misclassified_samples(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_NB.rename(columns={'index': 'index_NB'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_NB['index_NB']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output:\n",
    "```zsh\n",
    "The accuracy is 0.637\n",
    "The confusion matrix is \n",
    "[[ 468  577  385  617]\n",
    " [ 109 1444  208  206]\n",
    " [ 104  209 1511  162]\n",
    " [  73  160   94 1673]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, find the best hyperparameters using cross validation:\n",
    "kfold = StratifiedKFold(n_splits=5, random_state =42, shuffle=True)\n",
    "\n",
    "n_estimators = [10, 50, 100, 200, 500]\n",
    "max_depths = [10, 50, 100, 200]\n",
    "\n",
    "i=0\n",
    "start = time.time()\n",
    "t0 = time.time()\n",
    "accs_rf = np.zeros((5, len(n_estimators), len(max_depths)))\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    X_tt, X_ho = X_train[train_index], X_train[test_index]\n",
    "    y_tt, y_ho = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    for n in n_estimators:\n",
    "        for d in max_depths:\n",
    "            print(f\"start to process {n} and {d} in the {i+1}th fold\")\n",
    "            rf = RandomForestClassifier(n_estimators=n, max_depth=d)\n",
    "            rf.fit(X_tt, y_tt)\n",
    "            accs_rf[i, n_estimators.index(n), max_depths.index(d)] = accuracy_score(y_ho, rf.predict(X_ho))\n",
    "            print(f\"To process {n} and {d} in the {i+1}th fold, it takes {time.time()-t0} seconds\")\n",
    "            t0 = time.time()\n",
    "    i += 1\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"RandomForest\", \"RandomForest has finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best number of neighbors\n",
    "accs_rf_mean = accs_rf.mean(axis=0)\n",
    "best_n, best_d = np.unravel_index(np.argmax(accs_rf_mean), accs_rf_mean.shape)\n",
    "print(accs_rf_mean)\n",
    "\n",
    "print(f\"The best number of neighbors is {n_estimators[best_n]} and the best max depth is {max_depths[best_d]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(n_estimators, accs_rf_mean[:, best_d])\n",
    "\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest: Accuracy vs Number of Estimators')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(max_depths, accs_rf_mean[best_n])\n",
    "\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest: Accuracy vs Max Depth')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output\n",
    "rf = RandomForestClassifier(n_estimators=n_estimators[best_n], max_depth=max_depths[best_d])\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output:\n",
    "``` zsh\n",
    "The accuracy is 0.740875\n",
    "The confusion matrix is \n",
    "[[1020  351  279  397]\n",
    " [ 200 1570  135   62]\n",
    " [ 187   67 1673   59]\n",
    " [ 214   71   51 1664]]\n",
    " ```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the grid search to find the best hyperparameters for the random forest model\n",
    "\n",
    "param_grid = {'n_estimators': [500, 600, 700, 800, 900, 1000], 'max_depth': [10, 50, 100, 200]}\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best parameters are {grid_search.best_params_}\")\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"RandomForest\", f\"The best parameters are {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are {'max_depth': 200, 'n_estimators': 700}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.734\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[ 971  375  262  392]\n",
      " [ 202 1581  146   71]\n",
      " [ 190   82 1661   67]\n",
      " [ 212   77   52 1659]]\n",
      "F1 Score when we treat ELSE as negative: 0.8685714285714285\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.54321678 0.76840826 0.80611502 0.79207448]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "# the output\n",
    "\n",
    "#rf_best = RandomForestClassifier(**grid_search.best_params_)\n",
    "rf_best = RandomForestClassifier(n_estimators=700, max_depth=200)\n",
    "\n",
    "rf_best.fit(X_train, y_train)\n",
    "y_pred = rf_best.predict(X_test)\n",
    "\n",
    "get_accuracy(y_test, y_pred)\n",
    "mis_RF = get_misclassified_samples(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```zsh\n",
    "The accuracy is 0.74075\n",
    "The confusion matrix \n",
    "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
    "[[1010  363  277  397]\n",
    " [ 198 1568  144   57]\n",
    " [ 193   63 1666   64]\n",
    " [ 199   72   47 1682]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_RF.rename(columns={'index': 'index_RF'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_RF['index_RF']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: LDA and QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA\n",
    "kfold = StratifiedKFold(n_splits=5, random_state =42, shuffle=True)\n",
    "accs_LDA = np.zeros((5, 1))\n",
    "\n",
    "i = 0\n",
    "start = time.time()\n",
    "t0 = start\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    X_tt, X_ho = X_train[train_index], X_train[test_index]\n",
    "    y_tt, y_ho = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    print(f\"start to process LDA in the {i+1}th fold\")\n",
    "    lda.fit(X_tt, y_tt)\n",
    "    accs_LDA[i, 0] = accuracy_score(y_ho, lda.predict(X_ho))\n",
    "    print(f\"To process LDA in the {i+1}th fold, it takes {time.time()-t0} seconds\")\n",
    "    t0 = time.time()\n",
    "    i += 1\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"LDA\", \"LDA has finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accs_LDA = accs_LDA.mean(axis=0)\n",
    "print(mean_accs_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.69625\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[1384  224  173  219]\n",
      " [ 531 1306  105   58]\n",
      " [ 434   65 1455   46]\n",
      " [ 509   44   22 1425]]\n",
      "F1 Score when we treat ELSE as negative: 0.8124214683180757\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.5697818  0.71777961 0.77496671 0.76040555]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "# get accuracy\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)\n",
    "mis_LDA = get_misclassified_samples(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output:\n",
    "``` zsh\n",
    "The accuracy is 0.694125\n",
    "The confusion matrix is \n",
    "[[1380  244  174  249]\n",
    " [ 543 1258  118   48]\n",
    " [ 409   57 1469   51]\n",
    " [ 491   45   18 1446]]\n",
    " ```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_LDA.rename(columns={'index': 'index_LDA'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_LDA['index_LDA']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train QDA\n",
    "kfold = StratifiedKFold(n_splits=5, random_state =42, shuffle=True)\n",
    "accs_QDA = np.zeros((5, 1))\n",
    "\n",
    "i = 0\n",
    "start = time.time()\n",
    "t0 = start\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    X_tt, X_ho = X_train[train_index], X_train[test_index]\n",
    "    y_tt, y_ho = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    print(f\"start to process QDA in the {i+1}th fold\")\n",
    "    qda.fit(X_tt, y_tt)\n",
    "    accs_QDA[i, 0] = accuracy_score(y_ho, qda.predict(X_ho))\n",
    "    print(f\"To process QDA in the {i+1}th fold, it takes {time.time()-t0} seconds\")\n",
    "    t0 = time.time()\n",
    "    i += 1\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"QDA\", \"QDA has finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accs_QDA = accs_QDA.mean(axis=0)\n",
    "print(mean_accs_QDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.637625\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[ 520  304  357  819]\n",
      " [ 184 1232  185  399]\n",
      " [ 128  132 1588  152]\n",
      " [ 108   60   71 1761]]\n",
      "F1 Score when we treat ELSE as negative: 0.8545176110260337\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.3537415  0.66094421 0.75601047 0.6864159 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "# get accuracy\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "y_pred = qda.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)\n",
    "mis_QDA = get_misclassified_samples(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output:\n",
    "``` zsh\n",
    "The accuracy is 0.631875\n",
    "The confusion matrix is \n",
    "[[ 498  354  401  794]\n",
    " [ 158 1211  204  394]\n",
    " [ 129  120 1585  152]\n",
    " [ 126   46   67 1761]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_QDA.rename(columns={'index': 'index_QDA'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_QDA['index_QDA']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, find the best hyperparameters using cross validation:\n",
    "kfold = StratifiedKFold(n_splits=5, random_state =42, shuffle=True)\n",
    "n_estimators = [10, 50, 100, 200, 500]\n",
    "max_depths = [10, 50, 100, 200]\n",
    "\n",
    "i=0\n",
    "start = time.time()\n",
    "t0 = time.time()\n",
    "accs_gb = np.zeros((5, len(n_estimators), len(max_depths)))\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    X_tt, X_ho = X_train[train_index], X_train[test_index]\n",
    "    y_tt, y_ho = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    for n in n_estimators:\n",
    "        for d in max_depths:\n",
    "            print(f\"start to process {n} and {d} in the {i+1}th fold\")\n",
    "            gb = GradientBoostingClassifier(n_estimators=n, max_depth=d)\n",
    "            gb.fit(X_tt, y_tt)\n",
    "            accs_gb[i, n_estimators.index(n), max_depths.index(d)] = accuracy_score(y_ho, gb.predict(X_ho))\n",
    "            print(f\"To process {n} and {d} in the {i+1}th fold, it takes {time.time()-t0} seconds\")\n",
    "            t0 = time.time()\n",
    "    i += 1\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"GB\", \"GB has finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "``` zsh\n",
    "start to process 10 and 10 in the 1th fold\n",
    "To process 10 and 10 in the 1th fold, it takes 49.2708637714386 seconds\n",
    "start to process 10 and 50 in the 1th fold\n",
    "To process 10 and 50 in the 1th fold, it takes 184.7017343044281 seconds\n",
    "start to process 10 and 100 in the 1th fold\n",
    "To process 10 and 100 in the 1th fold, it takes 225.05927395820618 seconds\n",
    "start to process 10 and 200 in the 1th fold\n",
    "To process 10 and 200 in the 1th fold, it takes 225.06189227104187 seconds\n",
    "start to process 50 and 10 in the 1th fold\n",
    "To process 50 and 10 in the 1th fold, it takes 237.2459499835968 seconds\n",
    "start to process 50 and 50 in the 1th fold\n",
    "To process 50 and 50 in the 1th fold, it takes 926.7899940013885 seconds\n",
    "start to process 50 and 100 in the 1th fold\n",
    "To process 50 and 100 in the 1th fold, it takes 1125.3994789123535 seconds\n",
    "start to process 50 and 200 in the 1th fold\n",
    "To process 50 and 200 in the 1th fold, it takes 1138.620479106903 seconds\n",
    "start to process 100 and 10 in the 1th fold\n",
    "To process 100 and 10 in the 1th fold, it takes 480.2683551311493 seconds\n",
    "start to process 100 and 50 in the 1th fold\n",
    "To process 100 and 50 in the 1th fold, it takes 1830.9834089279175 seconds\n",
    "start to process 100 and 100 in the 1th fold\n",
    "To process 100 and 100 in the 1th fold, it takes 2087.277104139328 seconds\n",
    "start to process 100 and 200 in the 1th fold\n",
    "To process 100 and 200 in the 1th fold, it takes 2109.9916110038757 seconds\n",
    "start to process 200 and 10 in the 1th fold\n",
    "...\n",
    "start to process 500 and 100 in the 5th fold\n",
    "To process 500 and 100 in the 5th fold, it takes 2493.318783044815 seconds\n",
    "start to process 500 and 200 in the 5th fold\n",
    "To process 500 and 200 in the 5th fold, it takes 2527.178970813751 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "accs_gb_mean = accs_gb.mean(axis=0)\n",
    "print(accs_gb_mean)\n",
    "best_n, best_d = np.unravel_index(np.argmax(accs_gb_mean), accs_gb_mean.shape)\n",
    "print(f\"The best number of estimators is {n_estimators[best_n]} and the best max depth is {max_depths[best_d]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "``` zsh\n",
    "[[0.7168125  0.6551875  0.61746875 0.6205    ]\n",
    " [0.75153125 0.71215625 0.622      0.62478125]\n",
    " [0.75946875 0.7285     0.62328125 0.62659375]\n",
    " [0.76415625 0.736875   0.67715625 0.67771875]\n",
    " [0.770625   0.737      0.6783125  0.677125  ]]\n",
    "The best number of neighbors is 500 and the best max depth is 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(n_estimators, accs_gb_mean[:, best_d])\n",
    "\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Gradient Boosting: Accuracy vs Number of Estimators')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(max_depths, accs_gb_mean[best_n])\n",
    "\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Gradient Boosting: Accuracy vs Max Depth')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.76825\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[1188  283  219  310]\n",
      " [ 226 1616  102   56]\n",
      " [ 221   66 1674   39]\n",
      " [ 260   43   29 1668]]\n",
      "F1 Score when we treat ELSE as negative: 0.874514663362247\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.61001284 0.80638723 0.83200795 0.8190523 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "# Final output\n",
    "\n",
    "#gb = GradientBoostingClassifier(n_estimators=n_estimators[best_n], max_depth=max_depths[best_d])\n",
    "gb = GradientBoostingClassifier(n_estimators=500, max_depth=10)\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "get_accuracy(y_test, y_pred)\n",
    "mis_GB = get_misclassified_samples(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "``` zsh\n",
    "The accuracy is 0.773\n",
    "The confusion matrix \n",
    "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
    "[[1275  273  213  286]\n",
    " [ 228 1591  104   44]\n",
    " [ 233   61 1660   32]\n",
    " [ 271   47   24 1658]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_GB.rename(columns={'index': 'index_GB'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_GB['index_GB']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try larger number of estimators\n",
    "n_estimators = [1000, 2000, 5000]\n",
    "for n in n_estimators:\n",
    "    gb = GradientBoostingClassifier(n_estimators=n, max_depth=max_depths[best_d])\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred = gb.predict(X_test)\n",
    "    print(f\"The accuracy for {n} estimators is {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "duration = 1\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"GB\", \"GB has finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "``` zsh\n",
    "The accuracy for 1000 estimators is 0.7765\n",
    "The accuracy for 2000 estimators is 0.77475\n",
    "The accuracy for 5000 estimators is 0.776875\n",
    "```\n",
    "It does not significantly increase the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the xgboost model\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!Warning: It takes a long time to run, about 10 hours\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Train the model, find the best hyperparameters using grid search:\n",
    "param_grid = {'n_estimators': [10, 50, 100, 200, 500], 'max_depth': [10, 50, 100, 200], 'learning_rate': [0.01, 0.1, 0.3, 0.5]}\n",
    "xgb = XGBClassifier(early_stopping_rounds=10, eval_metric='mlogloss')\n",
    "\n",
    "grid_search = GridSearchCV(xgb, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train_encoded, eval_set=[(X_test, y_test_encoded)])\n",
    "\n",
    "print(f\"The best parameters are {grid_search.best_params_}\")\n",
    "\n",
    "duration = 3\n",
    "freq = 440\n",
    "os.system('play -nq -t coreaudio synth {} sine {}'.format(duration, freq))\n",
    "\n",
    "notify(\"XGBoost\", f\"The best parameters are {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```zsh\n",
    "The best parameters are {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 500}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.774125\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[1197  268  215  320]\n",
      " [ 207 1638  102   53]\n",
      " [ 204   78 1686   32]\n",
      " [ 248   48   32 1672]]\n",
      "F1 Score when we treat ELSE as negative: 0.8796113306982872\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.62085062 0.8125     0.83568773 0.82021094]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "#y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# the output\n",
    "#xgb_best = XGBClassifier(**grid_search.best_params_)\n",
    "xgb_best = XGBClassifier(learning_rate=0.1, max_depth = 10, n_estimators = 500)\n",
    "\n",
    "\n",
    "xgb_best.fit(X_train, y_train_encoded)\n",
    "y_pred_encoded = xgb_best.predict(X_test)\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "#y_test = label_encoder.inverse_transform(y_test_encoded)\n",
    "get_accuracy(y_test, y_pred)\n",
    "\n",
    "\n",
    "mis_XGB = get_misclassified_samples(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```zsh\n",
    "The accuracy is 0.77475\n",
    "The confusion matrix \n",
    "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
    "[[1224  289  230  304]\n",
    " [ 223 1589  106   49]\n",
    " [ 206   62 1686   32]\n",
    " [ 226   44   31 1699]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_XGB.rename(columns={'index': 'index_XGB'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_XGB['index_XGB']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the Keras API to build a neural network. We choose Keras for simplicity and ease of use.\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 200) (32000, 4)\n",
      "(8000, 200) (8000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Encode the labels\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_encoded = keras.utils.to_categorical(y_train_encoded, num_classes=4)\n",
    "y_test_encoded = keras.utils.to_categorical(y_test_encoded, num_classes=4)\n",
    "\n",
    "print(X_train.shape, y_train_encoded.shape)\n",
    "print(X_test.shape, y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the usual Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "# Creat early stopping callback\n",
    "early_stopping_nn = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=10, restore_best_weights=True, min_delta=0.001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The usual Neural Network model\n",
    "\n",
    "NNmodel = keras.models.Sequential()\n",
    "\n",
    "NNmodel.add(keras.layers.Flatten())\n",
    "NNmodel.add(keras.layers.Dense(128, activation='relu'))\n",
    "#Use dropout to prevent overfitting\n",
    "NNmodel.add(keras.layers.Dropout(0.4))\n",
    "NNmodel.add(keras.layers.Dense(64, activation='relu'))\n",
    "#Use dropout to prevent overfitting\n",
    "NNmodel.add(keras.layers.Dropout(0.3))\n",
    "NNmodel.add(keras.layers.Dense(4, activation = 'softmax'))\n",
    "\n",
    "NNmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NNmodel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3197 - loss: 1.3809 - val_accuracy: 0.4320 - val_loss: 1.3357\n",
      "Epoch 2/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4760 - loss: 1.2941 - val_accuracy: 0.6084 - val_loss: 1.1280\n",
      "Epoch 3/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5543 - loss: 1.0983 - val_accuracy: 0.6005 - val_loss: 0.9820\n",
      "Epoch 4/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5972 - loss: 0.9780 - val_accuracy: 0.6332 - val_loss: 0.9060\n",
      "Epoch 5/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6270 - loss: 0.9105 - val_accuracy: 0.6553 - val_loss: 0.8568\n",
      "Epoch 6/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6460 - loss: 0.8641 - val_accuracy: 0.6622 - val_loss: 0.8329\n",
      "Epoch 7/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6605 - loss: 0.8357 - val_accuracy: 0.6752 - val_loss: 0.8102\n",
      "Epoch 8/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6695 - loss: 0.8225 - val_accuracy: 0.6869 - val_loss: 0.7869\n",
      "Epoch 9/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6791 - loss: 0.7981 - val_accuracy: 0.6901 - val_loss: 0.7780\n",
      "Epoch 10/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6881 - loss: 0.7790 - val_accuracy: 0.7005 - val_loss: 0.7599\n",
      "Epoch 11/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6977 - loss: 0.7615 - val_accuracy: 0.7039 - val_loss: 0.7537\n",
      "Epoch 12/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6970 - loss: 0.7593 - val_accuracy: 0.7080 - val_loss: 0.7416\n",
      "Epoch 13/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7040 - loss: 0.7534 - val_accuracy: 0.7115 - val_loss: 0.7368\n",
      "Epoch 14/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7039 - loss: 0.7537 - val_accuracy: 0.7155 - val_loss: 0.7302\n",
      "Epoch 15/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7104 - loss: 0.7369 - val_accuracy: 0.7160 - val_loss: 0.7248\n",
      "Epoch 16/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7110 - loss: 0.7414 - val_accuracy: 0.7182 - val_loss: 0.7226\n",
      "Epoch 17/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7137 - loss: 0.7298 - val_accuracy: 0.7216 - val_loss: 0.7161\n",
      "Epoch 18/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7146 - loss: 0.7309 - val_accuracy: 0.7184 - val_loss: 0.7153\n",
      "Epoch 19/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7179 - loss: 0.7196 - val_accuracy: 0.7250 - val_loss: 0.7113\n",
      "Epoch 20/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7201 - loss: 0.7140 - val_accuracy: 0.7229 - val_loss: 0.7064\n",
      "Epoch 21/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7225 - loss: 0.7158 - val_accuracy: 0.7221 - val_loss: 0.7107\n",
      "Epoch 22/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7243 - loss: 0.7060 - val_accuracy: 0.7264 - val_loss: 0.7032\n",
      "Epoch 23/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7242 - loss: 0.7099 - val_accuracy: 0.7247 - val_loss: 0.6993\n",
      "Epoch 24/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7188 - loss: 0.7114 - val_accuracy: 0.7272 - val_loss: 0.6958\n",
      "Epoch 25/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7213 - loss: 0.7093 - val_accuracy: 0.7264 - val_loss: 0.6951\n",
      "Epoch 26/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7247 - loss: 0.7030 - val_accuracy: 0.7279 - val_loss: 0.6939\n",
      "Epoch 27/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7231 - loss: 0.7043 - val_accuracy: 0.7274 - val_loss: 0.6905\n",
      "Epoch 28/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7304 - loss: 0.6932 - val_accuracy: 0.7280 - val_loss: 0.6940\n",
      "Epoch 29/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7302 - loss: 0.6932 - val_accuracy: 0.7286 - val_loss: 0.6909\n",
      "Epoch 30/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7272 - loss: 0.6889 - val_accuracy: 0.7311 - val_loss: 0.6858\n",
      "Epoch 31/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7306 - loss: 0.6877 - val_accuracy: 0.7296 - val_loss: 0.6879\n",
      "Epoch 32/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7346 - loss: 0.6787 - val_accuracy: 0.7247 - val_loss: 0.6937\n",
      "Epoch 33/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7306 - loss: 0.6864 - val_accuracy: 0.7301 - val_loss: 0.6847\n",
      "Epoch 34/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7333 - loss: 0.6825 - val_accuracy: 0.7291 - val_loss: 0.6812\n",
      "Epoch 35/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7399 - loss: 0.6734 - val_accuracy: 0.7333 - val_loss: 0.6802\n",
      "Epoch 36/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7330 - loss: 0.6807 - val_accuracy: 0.7300 - val_loss: 0.6776\n",
      "Epoch 37/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7329 - loss: 0.6781 - val_accuracy: 0.7308 - val_loss: 0.6772\n",
      "Epoch 38/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7333 - loss: 0.6799 - val_accuracy: 0.7315 - val_loss: 0.6758\n",
      "Epoch 39/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7348 - loss: 0.6737 - val_accuracy: 0.7340 - val_loss: 0.6736\n",
      "Epoch 40/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7374 - loss: 0.6712 - val_accuracy: 0.7314 - val_loss: 0.6729\n",
      "Epoch 41/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7400 - loss: 0.6739 - val_accuracy: 0.7337 - val_loss: 0.6735\n",
      "Epoch 42/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7382 - loss: 0.6701 - val_accuracy: 0.7335 - val_loss: 0.6724\n",
      "Epoch 43/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7406 - loss: 0.6630 - val_accuracy: 0.7325 - val_loss: 0.6728\n",
      "Epoch 44/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7457 - loss: 0.6617 - val_accuracy: 0.7359 - val_loss: 0.6711\n",
      "Epoch 45/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7406 - loss: 0.6636 - val_accuracy: 0.7326 - val_loss: 0.6691\n",
      "Epoch 46/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7437 - loss: 0.6609 - val_accuracy: 0.7335 - val_loss: 0.6674\n",
      "Epoch 47/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7486 - loss: 0.6512 - val_accuracy: 0.7359 - val_loss: 0.6652\n",
      "Epoch 48/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7468 - loss: 0.6500 - val_accuracy: 0.7385 - val_loss: 0.6636\n",
      "Epoch 49/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7505 - loss: 0.6436 - val_accuracy: 0.7341 - val_loss: 0.6629\n",
      "Epoch 50/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7459 - loss: 0.6480 - val_accuracy: 0.7385 - val_loss: 0.6633\n",
      "Epoch 51/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7503 - loss: 0.6401 - val_accuracy: 0.7368 - val_loss: 0.6618\n",
      "Epoch 52/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7454 - loss: 0.6478 - val_accuracy: 0.7368 - val_loss: 0.6604\n",
      "Epoch 53/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7441 - loss: 0.6544 - val_accuracy: 0.7380 - val_loss: 0.6626\n",
      "Epoch 54/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7494 - loss: 0.6428 - val_accuracy: 0.7371 - val_loss: 0.6616\n",
      "Epoch 55/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7495 - loss: 0.6451 - val_accuracy: 0.7398 - val_loss: 0.6582\n",
      "Epoch 56/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7535 - loss: 0.6395 - val_accuracy: 0.7410 - val_loss: 0.6599\n",
      "Epoch 57/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7504 - loss: 0.6426 - val_accuracy: 0.7369 - val_loss: 0.6582\n",
      "Epoch 58/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7567 - loss: 0.6375 - val_accuracy: 0.7401 - val_loss: 0.6601\n",
      "Epoch 59/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7490 - loss: 0.6441 - val_accuracy: 0.7421 - val_loss: 0.6543\n",
      "Epoch 60/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7509 - loss: 0.6410 - val_accuracy: 0.7396 - val_loss: 0.6544\n",
      "Epoch 61/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7532 - loss: 0.6346 - val_accuracy: 0.7418 - val_loss: 0.6581\n",
      "Epoch 62/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7542 - loss: 0.6356 - val_accuracy: 0.7420 - val_loss: 0.6554\n",
      "Epoch 63/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7588 - loss: 0.6288 - val_accuracy: 0.7431 - val_loss: 0.6553\n",
      "Epoch 64/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7535 - loss: 0.6335 - val_accuracy: 0.7429 - val_loss: 0.6525\n",
      "Epoch 65/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7577 - loss: 0.6288 - val_accuracy: 0.7415 - val_loss: 0.6549\n",
      "Epoch 66/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7529 - loss: 0.6358 - val_accuracy: 0.7434 - val_loss: 0.6508\n",
      "Epoch 67/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7582 - loss: 0.6333 - val_accuracy: 0.7419 - val_loss: 0.6508\n",
      "Epoch 68/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7508 - loss: 0.6370 - val_accuracy: 0.7421 - val_loss: 0.6517\n",
      "Epoch 69/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7545 - loss: 0.6314 - val_accuracy: 0.7419 - val_loss: 0.6501\n",
      "Epoch 70/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7570 - loss: 0.6214 - val_accuracy: 0.7400 - val_loss: 0.6502\n",
      "Epoch 71/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7621 - loss: 0.6159 - val_accuracy: 0.7430 - val_loss: 0.6495\n",
      "Epoch 72/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7638 - loss: 0.6174 - val_accuracy: 0.7416 - val_loss: 0.6543\n",
      "Epoch 73/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7584 - loss: 0.6268 - val_accuracy: 0.7420 - val_loss: 0.6503\n",
      "Epoch 74/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7592 - loss: 0.6220 - val_accuracy: 0.7418 - val_loss: 0.6515\n",
      "Epoch 75/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7599 - loss: 0.6218 - val_accuracy: 0.7419 - val_loss: 0.6482\n",
      "Epoch 76/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7575 - loss: 0.6190 - val_accuracy: 0.7450 - val_loss: 0.6470\n",
      "Epoch 77/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7623 - loss: 0.6148 - val_accuracy: 0.7426 - val_loss: 0.6468\n",
      "Epoch 78/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7654 - loss: 0.6103 - val_accuracy: 0.7437 - val_loss: 0.6453\n",
      "Epoch 79/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7647 - loss: 0.6175 - val_accuracy: 0.7455 - val_loss: 0.6494\n",
      "Epoch 80/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7673 - loss: 0.6059 - val_accuracy: 0.7477 - val_loss: 0.6473\n",
      "Epoch 81/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7604 - loss: 0.6191 - val_accuracy: 0.7410 - val_loss: 0.6479\n",
      "Epoch 82/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7612 - loss: 0.6108 - val_accuracy: 0.7425 - val_loss: 0.6477\n",
      "Epoch 83/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7650 - loss: 0.6055 - val_accuracy: 0.7464 - val_loss: 0.6438\n",
      "Epoch 84/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7620 - loss: 0.6135 - val_accuracy: 0.7437 - val_loss: 0.6457\n",
      "Epoch 85/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7668 - loss: 0.6083 - val_accuracy: 0.7464 - val_loss: 0.6426\n",
      "Epoch 86/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7658 - loss: 0.6080 - val_accuracy: 0.7461 - val_loss: 0.6455\n",
      "Epoch 87/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7640 - loss: 0.6151 - val_accuracy: 0.7449 - val_loss: 0.6460\n",
      "Epoch 88/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7696 - loss: 0.5998 - val_accuracy: 0.7454 - val_loss: 0.6456\n",
      "Epoch 89/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7667 - loss: 0.6028 - val_accuracy: 0.7445 - val_loss: 0.6429\n",
      "Epoch 90/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7680 - loss: 0.5994 - val_accuracy: 0.7461 - val_loss: 0.6433\n",
      "Epoch 91/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7645 - loss: 0.6050 - val_accuracy: 0.7460 - val_loss: 0.6441\n",
      "Epoch 92/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7655 - loss: 0.5992 - val_accuracy: 0.7465 - val_loss: 0.6439\n",
      "Epoch 93/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7688 - loss: 0.6010 - val_accuracy: 0.7421 - val_loss: 0.6507\n",
      "Epoch 94/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7708 - loss: 0.5960 - val_accuracy: 0.7448 - val_loss: 0.6470\n",
      "Epoch 95/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7687 - loss: 0.6015 - val_accuracy: 0.7475 - val_loss: 0.6449\n",
      "Epoch 95: early stopping\n",
      "Restoring model weights from the end of the best epoch: 85.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "hist_nn = NNmodel.fit(X_train, y_train_encoded, epochs=200, batch_size=500, validation_split=0.2, validation_data=(X_test, y_test_encoded), callbacks=[early_stopping_nn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,728</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m200\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │        \u001b[38;5;34m25,728\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m4\u001b[0m)               │           \u001b[38;5;34m260\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">102,734</span> (401.31 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m102,734\u001b[0m (401.31 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,244</span> (133.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,244\u001b[0m (133.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,490</span> (267.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m68,490\u001b[0m (267.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NNmodel.summary()\n",
    "\n",
    "acc_nn = hist_nn.history['accuracy']\n",
    "val_nn = hist_nn.history['val_accuracy']\n",
    "epochs_nn = range(1, len(acc_nn) + 1)\n",
    "\n",
    "plt.plot(epochs_nn, acc_nn, '-', label='Training accuracy')\n",
    "plt.plot(epochs_nn, val_nn, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy: NN')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250us/step\n",
      "The accuracy is 0.746375\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[1078  326  284  312]\n",
      " [ 235 1573  134   58]\n",
      " [ 193   73 1697   37]\n",
      " [ 279   62   36 1623]]\n",
      "F1 Score when we treat ELSE as negative: 0.8666393778141629\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.56961691 0.7798711  0.8176343  0.80545906]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "y_pred_NN_encoded = np.argmax(NNmodel.predict(X_test), axis=1)\n",
    "y_pred_NN = label_encoder.inverse_transform(y_pred_NN_encoded)\n",
    "\n",
    "get_accuracy(y_test, y_pred_NN)\n",
    "mis_NN = get_misclassified_samples(y_test, y_pred_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_NN.rename(columns={'index': 'index_NN'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_NN['index_NN']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to stop early. The usual Neural Network method tends to overfit the training data while the validation accuracy does not change too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "# Creat early stopping callback\n",
    "early_stopping_cnn = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=10, restore_best_weights=True, min_delta=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaokangwang/Library/Caches/pypoetry/virtualenvs/erdosnewsfinanceproject-fYB74UfD-py3.12/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3168</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">405,632</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3168\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m405,632\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m260\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">414,276</span> (1.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m414,276\u001b[0m (1.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">414,276</span> (1.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m414,276\u001b[0m (1.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using CNN model (Adding Convolutional and MaxPooling layers)\n",
    "\n",
    "CNNmodel = keras.models.Sequential()\n",
    "\n",
    "# Add an additional dimension to the data for the convolutional layer\n",
    "CNNmodel.add(keras.layers.Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# Add the convolutional layer\n",
    "CNNmodel.add(keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Add the max pooling layer\n",
    "CNNmodel.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten the output\n",
    "CNNmodel.add(keras.layers.Flatten())\n",
    "\n",
    "# Add the dense layers\n",
    "CNNmodel.add(keras.layers.Dense(128, activation='relu'))\n",
    "CNNmodel.add(keras.layers.Dropout(0.4))\n",
    "CNNmodel.add(keras.layers.Dense(64, activation='relu'))\n",
    "CNNmodel.add(keras.layers.Dropout(0.3))\n",
    "CNNmodel.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "CNNmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "CNNmodel.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2946 - loss: 1.3798 - val_accuracy: 0.4882 - val_loss: 1.3098\n",
      "Epoch 2/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4450 - loss: 1.2656 - val_accuracy: 0.5249 - val_loss: 1.1194\n",
      "Epoch 3/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5300 - loss: 1.1049 - val_accuracy: 0.5935 - val_loss: 0.9981\n",
      "Epoch 4/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5838 - loss: 1.0031 - val_accuracy: 0.6323 - val_loss: 0.9336\n",
      "Epoch 5/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6183 - loss: 0.9345 - val_accuracy: 0.6456 - val_loss: 0.8853\n",
      "Epoch 6/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6377 - loss: 0.8954 - val_accuracy: 0.6411 - val_loss: 0.8717\n",
      "Epoch 7/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6447 - loss: 0.8730 - val_accuracy: 0.6671 - val_loss: 0.8316\n",
      "Epoch 8/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6660 - loss: 0.8455 - val_accuracy: 0.6760 - val_loss: 0.8165\n",
      "Epoch 9/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6672 - loss: 0.8272 - val_accuracy: 0.6804 - val_loss: 0.7968\n",
      "Epoch 10/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6763 - loss: 0.8199 - val_accuracy: 0.6852 - val_loss: 0.7929\n",
      "Epoch 11/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6777 - loss: 0.8053 - val_accuracy: 0.6873 - val_loss: 0.7807\n",
      "Epoch 12/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6863 - loss: 0.7961 - val_accuracy: 0.6916 - val_loss: 0.7732\n",
      "Epoch 13/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6906 - loss: 0.7862 - val_accuracy: 0.6941 - val_loss: 0.7701\n",
      "Epoch 14/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6911 - loss: 0.7786 - val_accuracy: 0.6995 - val_loss: 0.7582\n",
      "Epoch 15/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6988 - loss: 0.7682 - val_accuracy: 0.7006 - val_loss: 0.7555\n",
      "Epoch 16/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7028 - loss: 0.7563 - val_accuracy: 0.7038 - val_loss: 0.7466\n",
      "Epoch 17/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7048 - loss: 0.7524 - val_accuracy: 0.7021 - val_loss: 0.7557\n",
      "Epoch 18/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7041 - loss: 0.7479 - val_accuracy: 0.7109 - val_loss: 0.7348\n",
      "Epoch 19/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7079 - loss: 0.7402 - val_accuracy: 0.7110 - val_loss: 0.7295\n",
      "Epoch 20/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7060 - loss: 0.7367 - val_accuracy: 0.7105 - val_loss: 0.7274\n",
      "Epoch 21/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7083 - loss: 0.7299 - val_accuracy: 0.7141 - val_loss: 0.7235\n",
      "Epoch 22/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7139 - loss: 0.7258 - val_accuracy: 0.7147 - val_loss: 0.7172\n",
      "Epoch 23/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7204 - loss: 0.7165 - val_accuracy: 0.7164 - val_loss: 0.7169\n",
      "Epoch 24/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7208 - loss: 0.7154 - val_accuracy: 0.7194 - val_loss: 0.7122\n",
      "Epoch 25/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7217 - loss: 0.7103 - val_accuracy: 0.7194 - val_loss: 0.7122\n",
      "Epoch 26/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7264 - loss: 0.7049 - val_accuracy: 0.7235 - val_loss: 0.7047\n",
      "Epoch 27/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7230 - loss: 0.7060 - val_accuracy: 0.7203 - val_loss: 0.7088\n",
      "Epoch 28/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7248 - loss: 0.7073 - val_accuracy: 0.7230 - val_loss: 0.7003\n",
      "Epoch 29/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7260 - loss: 0.6958 - val_accuracy: 0.7240 - val_loss: 0.7019\n",
      "Epoch 30/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7299 - loss: 0.6943 - val_accuracy: 0.7278 - val_loss: 0.6950\n",
      "Epoch 31/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7325 - loss: 0.6837 - val_accuracy: 0.7254 - val_loss: 0.6935\n",
      "Epoch 32/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7287 - loss: 0.6882 - val_accuracy: 0.7296 - val_loss: 0.6894\n",
      "Epoch 33/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7312 - loss: 0.6796 - val_accuracy: 0.7283 - val_loss: 0.6928\n",
      "Epoch 34/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7336 - loss: 0.6741 - val_accuracy: 0.7271 - val_loss: 0.6885\n",
      "Epoch 35/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7303 - loss: 0.6822 - val_accuracy: 0.7259 - val_loss: 0.6900\n",
      "Epoch 36/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7388 - loss: 0.6677 - val_accuracy: 0.7369 - val_loss: 0.6832\n",
      "Epoch 37/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7338 - loss: 0.6697 - val_accuracy: 0.7311 - val_loss: 0.6863\n",
      "Epoch 38/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7384 - loss: 0.6668 - val_accuracy: 0.7356 - val_loss: 0.6854\n",
      "Epoch 39/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7377 - loss: 0.6689 - val_accuracy: 0.7296 - val_loss: 0.6859\n",
      "Epoch 40/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7381 - loss: 0.6578 - val_accuracy: 0.7330 - val_loss: 0.6798\n",
      "Epoch 41/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7455 - loss: 0.6554 - val_accuracy: 0.7356 - val_loss: 0.6848\n",
      "Epoch 42/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7424 - loss: 0.6504 - val_accuracy: 0.7358 - val_loss: 0.6765\n",
      "Epoch 43/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7480 - loss: 0.6476 - val_accuracy: 0.7345 - val_loss: 0.6785\n",
      "Epoch 44/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7399 - loss: 0.6527 - val_accuracy: 0.7333 - val_loss: 0.6763\n",
      "Epoch 45/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7444 - loss: 0.6539 - val_accuracy: 0.7364 - val_loss: 0.6710\n",
      "Epoch 46/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7454 - loss: 0.6511 - val_accuracy: 0.7351 - val_loss: 0.6703\n",
      "Epoch 47/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7515 - loss: 0.6311 - val_accuracy: 0.7340 - val_loss: 0.6753\n",
      "Epoch 48/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7515 - loss: 0.6328 - val_accuracy: 0.7384 - val_loss: 0.6686\n",
      "Epoch 49/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7523 - loss: 0.6329 - val_accuracy: 0.7380 - val_loss: 0.6716\n",
      "Epoch 50/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7513 - loss: 0.6334 - val_accuracy: 0.7364 - val_loss: 0.6695\n",
      "Epoch 51/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7517 - loss: 0.6299 - val_accuracy: 0.7386 - val_loss: 0.6699\n",
      "Epoch 52/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7510 - loss: 0.6308 - val_accuracy: 0.7359 - val_loss: 0.6747\n",
      "Epoch 53/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7562 - loss: 0.6298 - val_accuracy: 0.7385 - val_loss: 0.6685\n",
      "Epoch 54/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7525 - loss: 0.6329 - val_accuracy: 0.7308 - val_loss: 0.6731\n",
      "Epoch 55/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7558 - loss: 0.6319 - val_accuracy: 0.7347 - val_loss: 0.6691\n",
      "Epoch 56/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7560 - loss: 0.6250 - val_accuracy: 0.7385 - val_loss: 0.6690\n",
      "Epoch 57/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7565 - loss: 0.6200 - val_accuracy: 0.7400 - val_loss: 0.6675\n",
      "Epoch 58/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7613 - loss: 0.6121 - val_accuracy: 0.7399 - val_loss: 0.6649\n",
      "Epoch 59/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7548 - loss: 0.6257 - val_accuracy: 0.7410 - val_loss: 0.6637\n",
      "Epoch 60/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7614 - loss: 0.6113 - val_accuracy: 0.7383 - val_loss: 0.6699\n",
      "Epoch 61/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7609 - loss: 0.6109 - val_accuracy: 0.7375 - val_loss: 0.6663\n",
      "Epoch 62/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7666 - loss: 0.6024 - val_accuracy: 0.7390 - val_loss: 0.6605\n",
      "Epoch 63/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7641 - loss: 0.6018 - val_accuracy: 0.7439 - val_loss: 0.6594\n",
      "Epoch 64/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7625 - loss: 0.6104 - val_accuracy: 0.7375 - val_loss: 0.6690\n",
      "Epoch 65/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7617 - loss: 0.6090 - val_accuracy: 0.7408 - val_loss: 0.6557\n",
      "Epoch 66/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7638 - loss: 0.6002 - val_accuracy: 0.7347 - val_loss: 0.6736\n",
      "Epoch 67/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7643 - loss: 0.6008 - val_accuracy: 0.7419 - val_loss: 0.6672\n",
      "Epoch 68/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7668 - loss: 0.5914 - val_accuracy: 0.7399 - val_loss: 0.6613\n",
      "Epoch 69/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7686 - loss: 0.5934 - val_accuracy: 0.7372 - val_loss: 0.6623\n",
      "Epoch 70/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7709 - loss: 0.5958 - val_accuracy: 0.7404 - val_loss: 0.6553\n",
      "Epoch 71/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7718 - loss: 0.5902 - val_accuracy: 0.7389 - val_loss: 0.6647\n",
      "Epoch 72/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7711 - loss: 0.5924 - val_accuracy: 0.7426 - val_loss: 0.6628\n",
      "Epoch 73/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7752 - loss: 0.5873 - val_accuracy: 0.7405 - val_loss: 0.6605\n",
      "Epoch 74/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7697 - loss: 0.5887 - val_accuracy: 0.7381 - val_loss: 0.6629\n",
      "Epoch 75/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7722 - loss: 0.5900 - val_accuracy: 0.7414 - val_loss: 0.6690\n",
      "Epoch 76/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7730 - loss: 0.5839 - val_accuracy: 0.7425 - val_loss: 0.6585\n",
      "Epoch 77/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7714 - loss: 0.5878 - val_accuracy: 0.7399 - val_loss: 0.6618\n",
      "Epoch 78/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7754 - loss: 0.5819 - val_accuracy: 0.7426 - val_loss: 0.6649\n",
      "Epoch 79/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7742 - loss: 0.5784 - val_accuracy: 0.7416 - val_loss: 0.6578\n",
      "Epoch 80/200\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7751 - loss: 0.5778 - val_accuracy: 0.7448 - val_loss: 0.6579\n",
      "Epoch 80: early stopping\n",
      "Restoring model weights from the end of the best epoch: 70.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "hist_cnn = CNNmodel.fit(X_train, y_train_encoded, epochs=200, batch_size=500, validation_split=0.2, validation_data=(X_test, y_test_encoded), callbacks=[early_stopping_cnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3168</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">405,632</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3168\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m405,632\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m260\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,242,830</span> (4.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,242,830\u001b[0m (4.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">414,276</span> (1.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m414,276\u001b[0m (1.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">828,554</span> (3.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m828,554\u001b[0m (3.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CNNmodel.summary()\n",
    "\n",
    "acc_cnn = hist_cnn.history['accuracy']\n",
    "val_cnn = hist_cnn.history['val_accuracy']\n",
    "epochs_cnn = range(1, len(acc_cnn) + 1)\n",
    "\n",
    "plt.plot(epochs_cnn, acc_cnn, '-', label='Training accuracy')\n",
    "plt.plot(epochs_cnn, val_cnn, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy: CNN')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step\n",
      "The accuracy is 0.740375\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[1078  316  275  331]\n",
      " [ 276 1499  143   82]\n",
      " [ 203   66 1688   43]\n",
      " [ 261   47   34 1658]]\n",
      "F1 Score when we treat ELSE as negative: 0.8635692004596947\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.56469356 0.76323829 0.81545894 0.8060282 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "y_pred_CNN_encoded = np.argmax(CNNmodel.predict(X_test), axis=1)\n",
    "y_pred_CNN = label_encoder.inverse_transform(y_pred_CNN_encoded)\n",
    "\n",
    "get_accuracy(y_test, y_pred_CNN)\n",
    "mis_CNN = get_misclassified_samples(y_test, y_pred_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_CNN.rename(columns={'index': 'index_CNN'}, inplace=True)\n",
    "\n",
    "misclassified = pd.concat([misclassified, mis_CNN['index_CNN']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common indices of CNN and NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indes = np.intersect1d(mis_NN['index'], mis_CNN['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indes\n",
    "print(f\"The common percentage is {len(common_indes)/len(mis_NN)*100}%, and {len(common_indes)/len(mis_CNN)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use RNN model (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping callback\n",
    "early_stopping_RNN = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=10, restore_best_weights=True, min_delta=0.00001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RNN model\n",
    "\n",
    "RNN_model = keras.models.Sequential()\n",
    "\n",
    "# Add a reshape layer\n",
    "RNN_model.add(keras.layers.Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# Add an bidirectional LSTM layer\n",
    "RNN_model.add(keras.layers.Bidirectional(keras.layers.LSTM(units = 64, return_sequences=True, dropout=0.0, recurrent_dropout=0.0)))\n",
    "\n",
    "# Add another bidirectional LSTM layer\n",
    "RNN_model.add(keras.layers.Bidirectional(keras.layers.LSTM(units = 32, return_sequences=False, dropout=0.0, recurrent_dropout=0.0)))\n",
    "\n",
    "# Add the dense layers\n",
    "RNN_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "#RNN_model.add(keras.layers.Dropout(0.4))\n",
    "#RNN_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "#RNN_model.add(keras.layers.Dropout(0.3))\n",
    "# Add the output layer\n",
    "RNN_model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "RNN_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "hist_RNN = RNN_model.fit(X_train, y_train_encoded, epochs=200, batch_size=100, validation_split=0.2, validation_data=(X_test, y_test_encoded), callbacks=[early_stopping_RNN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model.summary()\n",
    "\n",
    "acc_RNN = hist_RNN.history['accuracy']\n",
    "val_RNN = hist_RNN.history['val_accuracy']\n",
    "epochs_RNN = range(1, len(acc_RNN) + 1)\n",
    "\n",
    "plt.plot(epochs_RNN, acc_RNN, '-', label='Training accuracy')\n",
    "plt.plot(epochs_RNN, val_RNN, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy: RNN')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RNN_encoded = np.argmax(RNN_model.predict(X_test), axis=1)\n",
    "y_pred_RNN = label_encoder.inverse_transform(y_pred_RNN_encoded)\n",
    "\n",
    "get_accuracy(y_test, y_pred_RNN)\n",
    "mis_RNN = get_misclassified_samples(y_test, y_pred_RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe instead of using TF-IDF, we can use the word2vec embedding since we do not need to specify the siginificance of each words anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9: Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the unclassified samples by any of the model\n",
    "complete_mask = misclassified.notna().all(axis=1)\n",
    "\n",
    "unclassified = misclassified[complete_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category        \n",
       "ELSE      24175     [ ' 1 0 ' ,   ' t o x i c ' ,   ' p e o p l e ...\n",
       "          57370     [ ' t e l e v i s i o n ' ,   ' a c a d e m y ...\n",
       "          140885    [ ' f o o d ' ,   ' p o e m ' ,   ' b e s t ' ...\n",
       "POLITICS  74271     [ ' k i m ' ,   ' d a v i s ' ,   ' l o s e s ...\n",
       "ELSE      130515    [ ' f e a r ' ,   ' g o i n g ' ,   ' b e t t ...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unclassified_token = mis_XGB['headline'].loc[complete_mask]\n",
    "unclassified_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[ ' t e l e v i s i o n ' ,   ' a c a d e m y ' ,   ' s u e ' ,   ' b l o c k ' ,   ' s a l e ' ,   ' w h i t n e y ' ,   ' h o u s t o n ' ,   ' e m m y ' ,   ' t e l e v i s i o n ' ,   ' a c a d e m y ' ,   ' s u i n g ' ,   ' s t o p ' ,   ' w h i t n e y ' ,   ' h o u s t o n ' ,   ' h e i r ' ,   ' s e l l i n g ' ,   ' e m m y ' ,   ' s t a t u e t t e ' ,   ' s a l e ' ,   ' w o u l d ' ,   ' t a r n i s h ' ]\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unclassified_token.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The category for this headline is blurred. Even humans cannot get the category correctly.\n",
    "\n",
    "On the other hand, the `Else` category contains some similar categories to the remaining three. We just brutally threw them away. To refine the classifier, it is important to merge all these simlar categories and then do the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index_KNN</th>\n",
       "      <th>index_SVM</th>\n",
       "      <th>index_log</th>\n",
       "      <th>index_NB</th>\n",
       "      <th>index_RF</th>\n",
       "      <th>index_LDA</th>\n",
       "      <th>index_QDA</th>\n",
       "      <th>index_GB</th>\n",
       "      <th>index_XGB</th>\n",
       "      <th>index_NN</th>\n",
       "      <th>index_CNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENTERTAINMENT</th>\n",
       "      <th>541</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ELSE</th>\n",
       "      <th>71019</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24175</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57370</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POLITICS</th>\n",
       "      <th>26459</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     index_KNN  index_SVM  index_log  index_NB  index_RF  \\\n",
       "ENTERTAINMENT 541            1          0          0         0         1   \n",
       "ELSE          71019          1          1          1         1         0   \n",
       "              24175          1          1          1         1         1   \n",
       "              57370          1          1          1         1         1   \n",
       "POLITICS      26459          1          1          1         1         1   \n",
       "\n",
       "                     index_LDA  index_QDA  index_GB  index_XGB  index_NN  \\\n",
       "ENTERTAINMENT 541            0          0         0          0         0   \n",
       "ELSE          71019          1          1         0          0         1   \n",
       "              24175          1          1         1          1         1   \n",
       "              57370          1          1         1          1         1   \n",
       "POLITICS      26459          1          1         0          0         1   \n",
       "\n",
       "                     index_CNN  \n",
       "ENTERTAINMENT 541            0  \n",
       "ELSE          71019          1  \n",
       "              24175          1  \n",
       "              57370          1  \n",
       "POLITICS      26459          1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the similarity of classification of two models\n",
    "binary_misclassified = misclassified.notna().astype(int)\n",
    "\n",
    "binary_misclassified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the hamming distance between two models\n",
    "def hamming_distance(col1, col2):\n",
    "    differences = sum(a != b for a, b in zip(col1, col2))\n",
    "    return 1-differences/len(col1)\n",
    "\n",
    "hamming_similarity_matrix = pd.DataFrame(index=binary_misclassified.columns, columns=binary_misclassified.columns)\n",
    "\n",
    "for col1 in binary_misclassified.columns:\n",
    "    for col2 in binary_misclassified.columns:\n",
    "        hamming_similarity_matrix.loc[col1, col2] = hamming_distance(binary_misclassified[col1], binary_misclassified[col2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_KNN</th>\n",
       "      <th>index_SVM</th>\n",
       "      <th>index_log</th>\n",
       "      <th>index_NB</th>\n",
       "      <th>index_RF</th>\n",
       "      <th>index_LDA</th>\n",
       "      <th>index_QDA</th>\n",
       "      <th>index_GB</th>\n",
       "      <th>index_XGB</th>\n",
       "      <th>index_NN</th>\n",
       "      <th>index_CNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index_KNN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.57578</td>\n",
       "      <td>0.538447</td>\n",
       "      <td>0.613299</td>\n",
       "      <td>0.429421</td>\n",
       "      <td>0.524517</td>\n",
       "      <td>0.44075</td>\n",
       "      <td>0.448366</td>\n",
       "      <td>0.43815</td>\n",
       "      <td>0.487184</td>\n",
       "      <td>0.477526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_SVM</th>\n",
       "      <td>0.57578</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.895431</td>\n",
       "      <td>0.761887</td>\n",
       "      <td>0.630386</td>\n",
       "      <td>0.820951</td>\n",
       "      <td>0.656204</td>\n",
       "      <td>0.688336</td>\n",
       "      <td>0.682949</td>\n",
       "      <td>0.77916</td>\n",
       "      <td>0.757244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_log</th>\n",
       "      <td>0.538447</td>\n",
       "      <td>0.895431</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.753529</td>\n",
       "      <td>0.656575</td>\n",
       "      <td>0.803306</td>\n",
       "      <td>0.711367</td>\n",
       "      <td>0.71081</td>\n",
       "      <td>0.708767</td>\n",
       "      <td>0.826152</td>\n",
       "      <td>0.789747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_NB</th>\n",
       "      <td>0.613299</td>\n",
       "      <td>0.761887</td>\n",
       "      <td>0.753529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.614413</td>\n",
       "      <td>0.656018</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.628529</td>\n",
       "      <td>0.626114</td>\n",
       "      <td>0.707467</td>\n",
       "      <td>0.696694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_RF</th>\n",
       "      <td>0.429421</td>\n",
       "      <td>0.630386</td>\n",
       "      <td>0.656575</td>\n",
       "      <td>0.614413</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.625186</td>\n",
       "      <td>0.57448</td>\n",
       "      <td>0.821322</td>\n",
       "      <td>0.822994</td>\n",
       "      <td>0.731612</td>\n",
       "      <td>0.744242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_LDA</th>\n",
       "      <td>0.524517</td>\n",
       "      <td>0.820951</td>\n",
       "      <td>0.803306</td>\n",
       "      <td>0.656018</td>\n",
       "      <td>0.625186</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.634287</td>\n",
       "      <td>0.699851</td>\n",
       "      <td>0.681092</td>\n",
       "      <td>0.769502</td>\n",
       "      <td>0.722325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_QDA</th>\n",
       "      <td>0.44075</td>\n",
       "      <td>0.656204</td>\n",
       "      <td>0.711367</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.57448</td>\n",
       "      <td>0.634287</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.594168</td>\n",
       "      <td>0.596954</td>\n",
       "      <td>0.689822</td>\n",
       "      <td>0.656018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_GB</th>\n",
       "      <td>0.448366</td>\n",
       "      <td>0.688336</td>\n",
       "      <td>0.71081</td>\n",
       "      <td>0.628529</td>\n",
       "      <td>0.821322</td>\n",
       "      <td>0.699851</td>\n",
       "      <td>0.594168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894688</td>\n",
       "      <td>0.787704</td>\n",
       "      <td>0.793648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_XGB</th>\n",
       "      <td>0.43815</td>\n",
       "      <td>0.682949</td>\n",
       "      <td>0.708767</td>\n",
       "      <td>0.626114</td>\n",
       "      <td>0.822994</td>\n",
       "      <td>0.681092</td>\n",
       "      <td>0.596954</td>\n",
       "      <td>0.894688</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783432</td>\n",
       "      <td>0.78789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_NN</th>\n",
       "      <td>0.487184</td>\n",
       "      <td>0.77916</td>\n",
       "      <td>0.826152</td>\n",
       "      <td>0.707467</td>\n",
       "      <td>0.731612</td>\n",
       "      <td>0.769502</td>\n",
       "      <td>0.689822</td>\n",
       "      <td>0.787704</td>\n",
       "      <td>0.783432</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.864413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_CNN</th>\n",
       "      <td>0.477526</td>\n",
       "      <td>0.757244</td>\n",
       "      <td>0.789747</td>\n",
       "      <td>0.696694</td>\n",
       "      <td>0.744242</td>\n",
       "      <td>0.722325</td>\n",
       "      <td>0.656018</td>\n",
       "      <td>0.793648</td>\n",
       "      <td>0.78789</td>\n",
       "      <td>0.864413</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index_KNN index_SVM index_log  index_NB  index_RF index_LDA  \\\n",
       "index_KNN       1.0   0.57578  0.538447  0.613299  0.429421  0.524517   \n",
       "index_SVM   0.57578       1.0  0.895431  0.761887  0.630386  0.820951   \n",
       "index_log  0.538447  0.895431       1.0  0.753529  0.656575  0.803306   \n",
       "index_NB   0.613299  0.761887  0.753529       1.0  0.614413  0.656018   \n",
       "index_RF   0.429421  0.630386  0.656575  0.614413       1.0  0.625186   \n",
       "index_LDA  0.524517  0.820951  0.803306  0.656018  0.625186       1.0   \n",
       "index_QDA   0.44075  0.656204  0.711367  0.656947   0.57448  0.634287   \n",
       "index_GB   0.448366  0.688336   0.71081  0.628529  0.821322  0.699851   \n",
       "index_XGB   0.43815  0.682949  0.708767  0.626114  0.822994  0.681092   \n",
       "index_NN   0.487184   0.77916  0.826152  0.707467  0.731612  0.769502   \n",
       "index_CNN  0.477526  0.757244  0.789747  0.696694  0.744242  0.722325   \n",
       "\n",
       "          index_QDA  index_GB index_XGB  index_NN index_CNN  \n",
       "index_KNN   0.44075  0.448366   0.43815  0.487184  0.477526  \n",
       "index_SVM  0.656204  0.688336  0.682949   0.77916  0.757244  \n",
       "index_log  0.711367   0.71081  0.708767  0.826152  0.789747  \n",
       "index_NB   0.656947  0.628529  0.626114  0.707467  0.696694  \n",
       "index_RF    0.57448  0.821322  0.822994  0.731612  0.744242  \n",
       "index_LDA  0.634287  0.699851  0.681092  0.769502  0.722325  \n",
       "index_QDA       1.0  0.594168  0.596954  0.689822  0.656018  \n",
       "index_GB   0.594168       1.0  0.894688  0.787704  0.793648  \n",
       "index_XGB  0.596954  0.894688       1.0  0.783432   0.78789  \n",
       "index_NN   0.689822  0.787704  0.783432       1.0  0.864413  \n",
       "index_CNN  0.656018  0.793648   0.78789  0.864413       1.0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the precision:\n",
    "\n",
    "- KNN: 54%\n",
    "- SVM: 70%\n",
    "- LOG: 70%\n",
    "- NB: 64%\n",
    "- RF: 74%\n",
    "- LDA: 69%\n",
    "- QDA: 63%\n",
    "- GB: 77%\n",
    "- XGB: 77%\n",
    "- NN: 74%\n",
    "- CNN: 74%\n",
    "\n",
    "Based on these information, we should try to use models where the hamming distance is small and itself the precision is high.\n",
    "\n",
    "So we choose: Logistic, RandomForest, XGB, CNN for a voting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Voting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have different classifiers using different shape of data and API, we will make a customized classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class customized_voting_classifier:\n",
    "    def __init__(self, X_train):\n",
    "        self.log = LogisticRegression(C=1000, solver='newton-cg')\n",
    "        self.rf = RandomForestClassifier(n_estimators=700, max_depth=200)\n",
    "        self.xgb = XGBClassifier(learning_rate=0.1, max_depth = 10, n_estimators = 500)\n",
    "        self.cnn = keras.models.Sequential()\n",
    "        # Add an additional dimension to the data for the convolutional layer\n",
    "        self.cnn.add(keras.layers.Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)))\n",
    "\n",
    "        # Add the convolutional layer\n",
    "        self.cnn.add(keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "\n",
    "        # Add the max pooling layer\n",
    "        self.cnn.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "        # Flatten the output\n",
    "        self.cnn.add(keras.layers.Flatten())\n",
    "\n",
    "        # Add the dense layers\n",
    "        self.cnn.add(keras.layers.Dense(128, activation='relu'))\n",
    "        self.cnn.add(keras.layers.Dropout(0.4))\n",
    "        self.cnn.add(keras.layers.Dense(64, activation='relu'))\n",
    "        self.cnn.add(keras.layers.Dropout(0.3))\n",
    "        self.cnn.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "        self.cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Lable encoder\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        print(\"Start to train the models: Logistic\")\n",
    "        self.log.fit(X_train, y_train)\n",
    "        print(\"Logistic Regression has been trained\")\n",
    "        print(\"Start to train the models: Random Forest\")\n",
    "        self.rf.fit(X_train, y_train)\n",
    "        print(\"Random Forest has been trained\")\n",
    "        # Encode the labels\n",
    "        y_train_encoded = self.label_encoder.fit_transform(y_train)\n",
    "        \n",
    "\n",
    "        # One-hot encode the labels\n",
    "        y_train_encoded_cnn = keras.utils.to_categorical(y_train_encoded, num_classes=4)\n",
    "       \n",
    "        print(\"Start to train the models: XGBoost\")\n",
    "        self.xgb.fit(X_train, y_train_encoded)\n",
    "        print(\"XGBoost has been trained\")\n",
    "        # Early stopping\n",
    "        print(\"Start to train the models: CNN\")\n",
    "        early_stopping_cnn = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=10, restore_best_weights=True, min_delta=0.0001, verbose=1)\n",
    "        self.cnn.fit(X_train, y_train_encoded_cnn, epochs=200, batch_size=500, validation_split=0.2, callbacks=[early_stopping_cnn], verbose=1)\n",
    "        print(\"CNN has been trained\")\n",
    "\n",
    "    def predict(self, X_test, arg = \"soft\", weights = [0.1, 0.2, 0.4, 0.3]):\n",
    "        # print(f\"The accuracy for Logistic Regression is {accuracy_score(y_test, self.log.predict(X_test))}\")\n",
    "        # print(f\"The accuracy for Random Forest is {accuracy_score(y_test, self.rf.predict(X_test))}\")\n",
    "        # print(f\"The accuracy for XGBoost is {accuracy_score(y_test, self.label_encoder.inverse_transform(self.xgb.predict(X_test)))}\")\n",
    "        # print(f\"The accuracy for CNN is {accuracy_score(y_test, self.label_encoder.inverse_transform(np.argmax(self.cnn.predict(X_test), axis=1)))}\")\n",
    "        ##############################\n",
    "        if arg == \"soft\":\n",
    "            log_prob = self.log.predict_proba(X_test)\n",
    "            rf_prob = self.rf.predict_proba(X_test)\n",
    "            xgb_prob = self.xgb.predict_proba(X_test)\n",
    "            cnn_prob = self.cnn.predict(X_test)\n",
    "\n",
    "            weight = weights\n",
    "\n",
    "            average_prob = log_prob*weight[0] + rf_prob*weight[1] + xgb_prob*weight[2] + cnn_prob*weight[3]\n",
    "\n",
    "            y_pred = self.label_encoder.inverse_transform(np.argmax(average_prob, axis=1))\n",
    "        elif arg == \"hard\":\n",
    "            log_pred = self.log.predict(X_test)\n",
    "            rf_pred = self.rf.predict(X_test)\n",
    "            xgb_pred = self.label_encoder.inverse_transform(self.xgb.predict(X_test))\n",
    "            cnn_pred = self.label_encoder.inverse_transform(np.argmax(self.cnn.predict(X_test), axis=1))\n",
    "\n",
    "            y_pred = []\n",
    "            for i in range(len(X_test)):\n",
    "                pred = [log_pred[i], rf_pred[i], xgb_pred[i], cnn_pred[i]]\n",
    "                y_pred.append(max(set(pred), key = pred.count))\n",
    "            y_pred = pd.Series(y_pred)\n",
    "            y_pred.index = y_test.index\n",
    "\n",
    "        else:\n",
    "            print(\"The arg should be either soft or hard\")\n",
    "            y_pred = None\n",
    "\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaokangwang/Library/Caches/pypoetry/virtualenvs/erdosnewsfinanceproject-fYB74UfD-py3.12/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train the models: Logistic\n",
      "Logistic Regression has been trained\n",
      "Start to train the models: Random Forest\n",
      "Random Forest has been trained\n",
      "Start to train the models: XGBoost\n",
      "XGBoost has been trained\n",
      "Start to train the models: CNN\n",
      "Epoch 1/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2708 - loss: 1.3839 - val_accuracy: 0.4613 - val_loss: 1.3566\n",
      "Epoch 2/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3940 - loss: 1.3285 - val_accuracy: 0.5255 - val_loss: 1.1972\n",
      "Epoch 3/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4953 - loss: 1.1783 - val_accuracy: 0.5478 - val_loss: 1.0823\n",
      "Epoch 4/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5406 - loss: 1.0781 - val_accuracy: 0.5892 - val_loss: 1.0058\n",
      "Epoch 5/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5797 - loss: 1.0073 - val_accuracy: 0.6225 - val_loss: 0.9509\n",
      "Epoch 6/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6049 - loss: 0.9579 - val_accuracy: 0.6420 - val_loss: 0.9049\n",
      "Epoch 7/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6272 - loss: 0.9126 - val_accuracy: 0.6475 - val_loss: 0.8768\n",
      "Epoch 8/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6371 - loss: 0.8885 - val_accuracy: 0.6489 - val_loss: 0.8677\n",
      "Epoch 9/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6452 - loss: 0.8762 - val_accuracy: 0.6716 - val_loss: 0.8309\n",
      "Epoch 10/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6560 - loss: 0.8492 - val_accuracy: 0.6772 - val_loss: 0.8188\n",
      "Epoch 11/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6614 - loss: 0.8388 - val_accuracy: 0.6798 - val_loss: 0.8098\n",
      "Epoch 12/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6801 - loss: 0.8112 - val_accuracy: 0.6827 - val_loss: 0.8038\n",
      "Epoch 13/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6719 - loss: 0.8129 - val_accuracy: 0.6783 - val_loss: 0.8020\n",
      "Epoch 14/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6788 - loss: 0.8067 - val_accuracy: 0.6934 - val_loss: 0.7815\n",
      "Epoch 15/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6871 - loss: 0.7888 - val_accuracy: 0.6930 - val_loss: 0.7907\n",
      "Epoch 16/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6862 - loss: 0.7841 - val_accuracy: 0.6942 - val_loss: 0.7742\n",
      "Epoch 17/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6925 - loss: 0.7795 - val_accuracy: 0.7047 - val_loss: 0.7669\n",
      "Epoch 18/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6930 - loss: 0.7760 - val_accuracy: 0.6919 - val_loss: 0.7717\n",
      "Epoch 19/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6984 - loss: 0.7645 - val_accuracy: 0.6952 - val_loss: 0.7702\n",
      "Epoch 20/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6970 - loss: 0.7605 - val_accuracy: 0.7072 - val_loss: 0.7537\n",
      "Epoch 21/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7041 - loss: 0.7491 - val_accuracy: 0.7036 - val_loss: 0.7520\n",
      "Epoch 22/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7050 - loss: 0.7380 - val_accuracy: 0.7083 - val_loss: 0.7543\n",
      "Epoch 23/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7101 - loss: 0.7359 - val_accuracy: 0.7083 - val_loss: 0.7459\n",
      "Epoch 24/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7053 - loss: 0.7365 - val_accuracy: 0.7148 - val_loss: 0.7396\n",
      "Epoch 25/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7125 - loss: 0.7299 - val_accuracy: 0.7155 - val_loss: 0.7333\n",
      "Epoch 26/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7142 - loss: 0.7282 - val_accuracy: 0.7127 - val_loss: 0.7335\n",
      "Epoch 27/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7179 - loss: 0.7147 - val_accuracy: 0.7136 - val_loss: 0.7369\n",
      "Epoch 28/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7190 - loss: 0.7129 - val_accuracy: 0.7139 - val_loss: 0.7279\n",
      "Epoch 29/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7255 - loss: 0.7102 - val_accuracy: 0.7181 - val_loss: 0.7235\n",
      "Epoch 30/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7251 - loss: 0.6980 - val_accuracy: 0.7172 - val_loss: 0.7241\n",
      "Epoch 31/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7234 - loss: 0.7060 - val_accuracy: 0.7177 - val_loss: 0.7240\n",
      "Epoch 32/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7260 - loss: 0.6940 - val_accuracy: 0.7184 - val_loss: 0.7214\n",
      "Epoch 33/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7274 - loss: 0.6910 - val_accuracy: 0.7209 - val_loss: 0.7224\n",
      "Epoch 34/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7314 - loss: 0.6892 - val_accuracy: 0.7234 - val_loss: 0.7147\n",
      "Epoch 35/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7252 - loss: 0.6918 - val_accuracy: 0.7248 - val_loss: 0.7143\n",
      "Epoch 36/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7319 - loss: 0.6795 - val_accuracy: 0.7197 - val_loss: 0.7223\n",
      "Epoch 37/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7260 - loss: 0.6865 - val_accuracy: 0.7212 - val_loss: 0.7136\n",
      "Epoch 38/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7320 - loss: 0.6803 - val_accuracy: 0.7239 - val_loss: 0.7106\n",
      "Epoch 39/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7334 - loss: 0.6822 - val_accuracy: 0.7242 - val_loss: 0.7095\n",
      "Epoch 40/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7330 - loss: 0.6738 - val_accuracy: 0.7241 - val_loss: 0.7070\n",
      "Epoch 41/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7291 - loss: 0.6795 - val_accuracy: 0.7289 - val_loss: 0.7000\n",
      "Epoch 42/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7412 - loss: 0.6644 - val_accuracy: 0.7262 - val_loss: 0.7023\n",
      "Epoch 43/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7369 - loss: 0.6693 - val_accuracy: 0.7256 - val_loss: 0.7009\n",
      "Epoch 44/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7382 - loss: 0.6676 - val_accuracy: 0.7281 - val_loss: 0.7061\n",
      "Epoch 45/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7442 - loss: 0.6539 - val_accuracy: 0.7300 - val_loss: 0.6929\n",
      "Epoch 46/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7405 - loss: 0.6615 - val_accuracy: 0.7256 - val_loss: 0.6993\n",
      "Epoch 47/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7509 - loss: 0.6446 - val_accuracy: 0.7284 - val_loss: 0.7012\n",
      "Epoch 48/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7466 - loss: 0.6513 - val_accuracy: 0.7306 - val_loss: 0.6929\n",
      "Epoch 49/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7456 - loss: 0.6494 - val_accuracy: 0.7333 - val_loss: 0.6918\n",
      "Epoch 50/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7437 - loss: 0.6518 - val_accuracy: 0.7280 - val_loss: 0.7009\n",
      "Epoch 51/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7568 - loss: 0.6311 - val_accuracy: 0.7262 - val_loss: 0.6965\n",
      "Epoch 52/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7478 - loss: 0.6460 - val_accuracy: 0.7339 - val_loss: 0.6893\n",
      "Epoch 53/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7495 - loss: 0.6455 - val_accuracy: 0.7297 - val_loss: 0.6957\n",
      "Epoch 54/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7532 - loss: 0.6267 - val_accuracy: 0.7325 - val_loss: 0.6921\n",
      "Epoch 55/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7493 - loss: 0.6395 - val_accuracy: 0.7359 - val_loss: 0.6862\n",
      "Epoch 56/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7547 - loss: 0.6294 - val_accuracy: 0.7327 - val_loss: 0.6906\n",
      "Epoch 57/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7528 - loss: 0.6323 - val_accuracy: 0.7367 - val_loss: 0.6873\n",
      "Epoch 58/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7573 - loss: 0.6237 - val_accuracy: 0.7342 - val_loss: 0.6899\n",
      "Epoch 59/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7528 - loss: 0.6284 - val_accuracy: 0.7339 - val_loss: 0.6865\n",
      "Epoch 60/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7542 - loss: 0.6267 - val_accuracy: 0.7353 - val_loss: 0.6844\n",
      "Epoch 61/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7554 - loss: 0.6232 - val_accuracy: 0.7355 - val_loss: 0.6847\n",
      "Epoch 62/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7624 - loss: 0.6165 - val_accuracy: 0.7356 - val_loss: 0.6950\n",
      "Epoch 63/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7618 - loss: 0.6069 - val_accuracy: 0.7312 - val_loss: 0.6849\n",
      "Epoch 64/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7574 - loss: 0.6192 - val_accuracy: 0.7356 - val_loss: 0.6847\n",
      "Epoch 65/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7572 - loss: 0.6208 - val_accuracy: 0.7339 - val_loss: 0.6931\n",
      "Epoch 66/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7605 - loss: 0.6103 - val_accuracy: 0.7397 - val_loss: 0.6818\n",
      "Epoch 67/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7646 - loss: 0.6084 - val_accuracy: 0.7355 - val_loss: 0.6846\n",
      "Epoch 68/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7686 - loss: 0.5998 - val_accuracy: 0.7377 - val_loss: 0.6833\n",
      "Epoch 69/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7671 - loss: 0.6007 - val_accuracy: 0.7358 - val_loss: 0.6848\n",
      "Epoch 70/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7593 - loss: 0.6049 - val_accuracy: 0.7353 - val_loss: 0.6906\n",
      "Epoch 71/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7640 - loss: 0.6080 - val_accuracy: 0.7414 - val_loss: 0.6883\n",
      "Epoch 72/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7648 - loss: 0.6038 - val_accuracy: 0.7362 - val_loss: 0.6800\n",
      "Epoch 73/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7655 - loss: 0.6013 - val_accuracy: 0.7380 - val_loss: 0.6832\n",
      "Epoch 74/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7681 - loss: 0.5912 - val_accuracy: 0.7369 - val_loss: 0.6824\n",
      "Epoch 75/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7694 - loss: 0.5876 - val_accuracy: 0.7342 - val_loss: 0.6960\n",
      "Epoch 76/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7676 - loss: 0.5905 - val_accuracy: 0.7352 - val_loss: 0.6891\n",
      "Epoch 77/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7669 - loss: 0.5959 - val_accuracy: 0.7384 - val_loss: 0.6908\n",
      "Epoch 78/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7732 - loss: 0.5855 - val_accuracy: 0.7330 - val_loss: 0.6913\n",
      "Epoch 79/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7693 - loss: 0.5890 - val_accuracy: 0.7387 - val_loss: 0.6771\n",
      "Epoch 80/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7663 - loss: 0.5961 - val_accuracy: 0.7391 - val_loss: 0.6760\n",
      "Epoch 81/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7730 - loss: 0.5830 - val_accuracy: 0.7359 - val_loss: 0.6855\n",
      "Epoch 82/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7713 - loss: 0.5877 - val_accuracy: 0.7392 - val_loss: 0.6832\n",
      "Epoch 83/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7707 - loss: 0.5829 - val_accuracy: 0.7377 - val_loss: 0.6841\n",
      "Epoch 84/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7768 - loss: 0.5776 - val_accuracy: 0.7394 - val_loss: 0.6856\n",
      "Epoch 85/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7753 - loss: 0.5790 - val_accuracy: 0.7369 - val_loss: 0.6821\n",
      "Epoch 86/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7779 - loss: 0.5753 - val_accuracy: 0.7344 - val_loss: 0.6899\n",
      "Epoch 87/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7713 - loss: 0.5767 - val_accuracy: 0.7408 - val_loss: 0.6853\n",
      "Epoch 88/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7764 - loss: 0.5749 - val_accuracy: 0.7384 - val_loss: 0.6864\n",
      "Epoch 89/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7829 - loss: 0.5654 - val_accuracy: 0.7377 - val_loss: 0.6811\n",
      "Epoch 90/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7790 - loss: 0.5608 - val_accuracy: 0.7375 - val_loss: 0.6866\n",
      "Epoch 90: early stopping\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "CNN has been trained\n"
     ]
    }
   ],
   "source": [
    "voting = customized_voting_classifier(X_train)\n",
    "voting.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step\n",
      "The accuracy is 0.763875\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[1034  292  273  401]\n",
      " [ 201 1572  148   79]\n",
      " [ 157   46 1744   53]\n",
      " [ 175   41   23 1761]]\n",
      "F1 Score when we treat ELSE as negative: 0.8794337649802944\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.5797589  0.79574791 0.83285578 0.82021425]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "voting_pred = voting.predict(X_test, arg=\"hard\")\n",
    "get_accuracy(y_test, voting_pred)\n",
    "mis_voting_hard = get_misclassified_samples(y_test, voting_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step\n",
      "Maximum accuracy: 0.777\n",
      "Best weights: [0.2, 0.2, 0.6000000000000001, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track best weights and accuracy\n",
    "best_acc = 0\n",
    "best_weights = None\n",
    "acc_voting = []\n",
    "\n",
    "# Iterate over possible weights\n",
    "for i in np.arange(0, 1.1, 0.1):\n",
    "    for j in np.arange(0, 1.1 - i, 0.1):\n",
    "        for k in np.arange(0, 1.1 - i - j, 0.1):\n",
    "            weights = [i, j, k, 1 - i - j - k]\n",
    "            voting_pred = voting.predict(X_test, arg=\"soft\", weights=weights)\n",
    "            acc = accuracy_score(y_test, voting_pred)\n",
    "            acc_voting.append((acc, weights))\n",
    "            \n",
    "            # Update best weights if current accuracy is higher\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_weights = weights\n",
    "\n",
    "# Get the maximum accuracy and corresponding weights\n",
    "max_acc, max_weights = max(acc_voting, key=lambda x: x[0])\n",
    "\n",
    "print(f\"Maximum accuracy: {max_acc}\")\n",
    "print(f\"Best weights: {max_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step\n",
      "The accuracy is 0.77675\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[1177  271  221  331]\n",
      " [ 199 1641  107   53]\n",
      " [ 200   70 1698   32]\n",
      " [ 224   46   32 1698]]\n",
      "F1 Score when we treat ELSE as negative: 0.8814754098360655\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.61947368 0.81479643 0.83686545 0.82547399]\n"
     ]
    }
   ],
   "source": [
    "voting_pred = voting.predict(X_test, arg=\"soft\", weights=[0.2, 0.3, 0.5, 0.0])\n",
    "get_accuracy(y_test, voting_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that hard voting sometimes is not good enough. We prefer using the soft with a slightly change of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step\n",
      "The accuracy is 0.776625\n",
      "The confusion matrix \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] is \n",
      "[[1163  284  232  321]\n",
      " [ 201 1630  113   56]\n",
      " [ 195   57 1712   36]\n",
      " [ 228   39   25 1708]]\n",
      "F1 Score when we treat ELSE as negative: 0.880373372635716\n",
      "F1 Score for everclass: \n",
      "['ELSE', 'ENTERTAINMENT', 'POLITICS', 'WELLNESS'] \n",
      "[0.6142065  0.81296758 0.83880451 0.82892502]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/txq0vh11623_5lrnfw_cpmsh0000gn/T/ipykernel_3591/2370515181.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  results = {'index': misclassified_idx, 'true': y_test[misclassified_idx], 'pred': y_pred[misclassified_idx], 'headline': sampled_test['headline_summary_tokenized'].iloc[misclassified_idx].apply(lambda x: ' '.join(x))}\n"
     ]
    }
   ],
   "source": [
    "voting_pred = voting.predict(X_test, arg=\"soft\", weights=[0.2, 0.2, 0.4, 0.2])\n",
    "get_accuracy(y_test, voting_pred)\n",
    "mis_voting_soft = get_misclassified_samples(y_test, voting_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/xiaokangwang/Documents/PycharmProjects/Projects for Erdos 2024 fall/data_set/model_news/classification_headline.joblib']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fitted model\n",
    "votting_model_path = os.path.join(config['info']['local_data_path'], 'model_news', config['news_model']['input']['classification_headlines'])\n",
    "joblib.dump(voting, votting_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good enough to seperate the lables except 'ELSE'. And this is beacuse our choice of 'ELSE' is quite arbitary, the f1 score of this class is relatively low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model to predict the ALL_news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the political data\n",
    "political_path = os.path.join(config['info']['local_data_path'], 'data_clean', config['news_preprocessing']['output']['all_news_politic_cleaned_file'])\n",
    "political = pd.read_csv(political_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>publication</th>\n",
       "      <th>summary</th>\n",
       "      <th>token</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-07 00:00:00</td>\n",
       "      <td>U.S. lawmakers ask for disclosure of number of...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>WASHINGTON (Reuters) - A U.S. congressional co...</td>\n",
       "      <td>['washington', 'reuters', 'congressional', 'co...</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-16 00:00:00</td>\n",
       "      <td>Trump keeping options open as Republican feud ...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>WASHINGTON (Reuters) - Like the deal-maker he ...</td>\n",
       "      <td>['washington', 'reuters', 'like', 'say', 'pres...</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-18 00:00:00</td>\n",
       "      <td>Trump tells anti-abortion marchers he will sup...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>['washington', 'reuters', 'president', 'donald...</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-21 00:00:00</td>\n",
       "      <td>Exclusive: Trump considering fracking mogul Ha...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>CLEVELAND (Reuters) - Republican presidential ...</td>\n",
       "      <td>['cleveland', 'reuters', 'republican', 'presid...</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-06 00:00:00</td>\n",
       "      <td>White House chief of staff's personal cellphon...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>WASHINGTON (Reuters) - White House officials b...</td>\n",
       "      <td>['washington', 'reuters', 'white', 'house', 'o...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              title  \\\n",
       "0  2017-04-07 00:00:00  U.S. lawmakers ask for disclosure of number of...   \n",
       "1  2017-10-16 00:00:00  Trump keeping options open as Republican feud ...   \n",
       "2  2019-01-18 00:00:00  Trump tells anti-abortion marchers he will sup...   \n",
       "3  2016-07-21 00:00:00  Exclusive: Trump considering fracking mogul Ha...   \n",
       "4  2017-10-06 00:00:00  White House chief of staff's personal cellphon...   \n",
       "\n",
       "    section publication                                            summary  \\\n",
       "0  Politics     Reuters  WASHINGTON (Reuters) - A U.S. congressional co...   \n",
       "1  Politics     Reuters  WASHINGTON (Reuters) - Like the deal-maker he ...   \n",
       "2  Politics     Reuters  WASHINGTON (Reuters) - U.S. President Donald T...   \n",
       "3  Politics     Reuters  CLEVELAND (Reuters) - Republican presidential ...   \n",
       "4  Politics     Reuters  WASHINGTON (Reuters) - White House officials b...   \n",
       "\n",
       "                                               token  word_count  \n",
       "0  ['washington', 'reuters', 'congressional', 'co...         402  \n",
       "1  ['washington', 'reuters', 'like', 'say', 'pres...         575  \n",
       "2  ['washington', 'reuters', 'president', 'donald...         421  \n",
       "3  ['cleveland', 'reuters', 'republican', 'presid...         610  \n",
       "4  ['washington', 'reuters', 'white', 'house', 'o...         174  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only the first 100 tokens\n",
    "political_headlines = political['token'].apply(lambda x: x.split()[2:102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['congressional',, 'committee',, 'friday',, 'a...\n",
       "1    ['like',, 'say',, 'president',, 'donald',, 'tr...\n",
       "2    ['president',, 'donald',, 'trump',, 'spoke',, ...\n",
       "3    ['republican',, 'presidential',, 'candidate',,...\n",
       "4    ['white',, 'house',, 'official',, 'believe',, ...\n",
       "Name: token, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the tfidf vectorizer to transform the data\n",
    "political_headlines_joined = political_headlines.apply(lambda x: ' '.join(x))\n",
    "political_tfidf = nmf.transform(tfidf.transform(political_headlines_joined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 572us/step\n"
     ]
    }
   ],
   "source": [
    "pred = voting.predict(political_tfidf, arg=\"soft\", weights=[0.2, 0.2, 0.4, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of the true politic detection is 96.7513493072983%\n"
     ]
    }
   ],
   "source": [
    "pd.Series(pred).describe()\n",
    "print(f\"The percentage of the true politic detection is {pd.Series(pred).value_counts(normalize=True)['POLITICS']*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdosnewsfinanceproject-fYB74UfD-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
