{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for the All news data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add ../src to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('__file__'), '../src'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import the parse config function to parse the .toml file\n",
    "from utils.config_tool import parse_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example dataset\n",
    "config_file = \"../config/predict_stock_w_news.toml\"\n",
    "config = parse_config(config_file)\n",
    "\n",
    "df = pd.read_csv(os.path.join(config['info']['local_data_path'],'data_raw', config['news_ingestion']['input'][\"all_news_eg1_file\"]))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of news articles per year\n",
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of news articles per year\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.bar(df['year'].value_counts().index, df['year'].value_counts())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This first 10000 rows in the files the time is almost evenly distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now looking at the specific time count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.bar(df['date'].value_counts().index, df['date'].value_counts().values)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that there are still labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of news articles per section\n",
    "df['section'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,40))\n",
    "\n",
    "sns.barplot(y=df['section'].value_counts().index, x=df['section'].value_counts().values)\n",
    "\n",
    "plt.yticks(rotation = 0, fontsize = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But there are still some nan lables. Note that nan is not of string type, it is of float type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the type of the NAN values in the section column\n",
    "print(df.iloc[0]['section'], type(df.iloc[0]['section']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of NAN values in the section column\n",
    "df[df['section'].isna()].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the publisher column\n",
    "print(df['publication'].value_counts())\n",
    "print(df['publication'].value_counts().sum())\n",
    "print(df['publication'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.pie(df['publication'].value_counts().values, autopct='%2.0f%%', \n",
    "                labels=df['publication'].value_counts().index)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is not of the same distribution as the whole data set as indicated in the description of the whole data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipline test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Drop the link and the time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'author', 'year', 'month', 'day', 'url']\n",
    "\n",
    "df = df.drop(columns_to_drop,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Select the publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is just an example, I don't need to do it now\n",
    "\n",
    "publisher = {'Vox'}\n",
    "print(type(publisher))\n",
    "df[df['publication'].isin(publisher)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Remove the null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the null values in the dataframe\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is no null element in title and article. It is OK for author and section to be null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['author'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like they are also resonable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['title'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['article'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['article'], inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, there are no empty articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Add column of word counting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count of the articles\n",
    "df['word_count'] = df['article'].apply(lambda x: len(x.split()))\n",
    "df['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "sns.displot(df['word_count'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['article'][df['article'].apply(lambda x: len(x.split())) <= 10])\n",
    "#print(len(df['article'][df['article'].apply(lambda x: len(x.split())) <= 30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there contains some advertisments and one word news, which is not so good for clustering task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Get the first 100 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_words(article_text):\n",
    "    # Split the article text into words\n",
    "    words = article_text.split()\n",
    "    # Get the first 100 words\n",
    "    first_100_words = words[:100]\n",
    "    # Join them back into a string\n",
    "    result = ' '.join(first_100_words)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['article'].apply(get_first_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artical_0 = df['article'][0]\n",
    "print(artical_0)\n",
    "print(len(df['summary'][0].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Download 'punkt' if you haven't already\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Append the NLTK data path\n",
    "nltk.data.path.append('/Users/xiaokangwang/nltk_data')\n",
    "\n",
    "# Download 'punkt' if you haven't already\n",
    "nltk.download('punkt', download_dir='/Users/xiaokangwang/nltk_data')\n",
    "\n",
    "# Verify the NLTK data path\n",
    "print(nltk.data.path)\n",
    "\n",
    "# Download 'punkt_tab' if you haven't already\n",
    "nltk.download('punkt_tab', download_dir='/Users/xiaokangwang/nltk_data')\n",
    "\n",
    "# Verify the 'punkt' tokenizer\n",
    "import os\n",
    "print(os.listdir('/Users/xiaokangwang/nltk_data/tokenizers/punkt'))\n",
    "\n",
    "# Load the 'punkt' tokenizer explicitly\n",
    "nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the word_tokenizer with the split method\n",
    "text = word_tokenize(df['summary'][0])\n",
    "print(len(text), text)\n",
    "print(len(df['summary'][0].split()), df['summary'][0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".split() and word_tokenize are basically the same thing, but tokenize are better. The only problem is that tokenize will include punctuations, so we better removed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_words_with_no_punctuation(text):\n",
    "    words = word_tokenize(text, language=\"english\")\n",
    "    words_no_punctuation = [word.lower() for word in words if word.isalnum()]\n",
    "    return words_no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(get_tokenized_words_with_no_punctuation(df['summary'][0])),get_tokenized_words_with_no_punctuation(df['summary'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Deleting the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    words_no_stop_words = [word for word in words if word not in stop_words]\n",
    "    return words_no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(remove_stop_words(get_tokenized_words_with_no_punctuation(df['summary'][0]))),remove_stop_words(get_tokenized_words_with_no_punctuation(df['summary'][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(article_text):\n",
    "    words = get_tokenized_words_with_no_punctuation(article_text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_words'] = df['article'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_words'] = df['tokenized_words'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('article', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_words'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7:NER transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy to extract named entities\n",
    "def extract_ner_features(text):\n",
    "    doc = nlp(text)\n",
    "    entity_counts = {\n",
    "        \"PERSON\": 0,\n",
    "        \"ORG\": 0,\n",
    "        \"GPE\": 0,\n",
    "        \"EVENT\": 0,\n",
    "        \"PRODUCT\": 0\n",
    "    }\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entity_counts:\n",
    "            entity_counts[ent.label_] += 1\n",
    "    return pd.Series(entity_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the extracted features to the dataframe\n",
    "\n",
    "A = df['tokenized_words'].apply(lambda x: extract_ner_features(' '.join(x)))\n",
    "print(A.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.join(A.apply(pd.Series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdosnewsfinanceproject-fYB74UfD-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
