############################################
# pipeline for predicting stock price with news
# set true to run the pipeline, false to skip the pipeline
[pipeline]
# finance
fin_ingestion = false
fin_processing = false
fin_model = true
# news
news_ingestion = false
news_processing = false
news_model = false
# prediction
predict_model = false
predict_evaluation = false


############################################
[info]
# local path to store all data (change to your local path)
local_data_path = "/Users/xiaokangwang/Documents/PycharmProjects/Projects for Erdos 2024 fall/data_set"
#local_data_path = "/Users/xwpeng/Projects/Erdos_NewsFin/data"

# directories names for data files in the Google Drive
# data_raw: raw stock and news data
# data_clean: cleaned and processed stock and news data, which can be used for modeling directly
# model_fin: finance model files and the outputs
# model_news: news model files and the outputs
# model_pre_train: preditive model files and the outputs
# model_pre_eval: evaluation results for the predictive models
dirs_names = ["logs", "config_archive", "data_raw", "data_clean", "model_fin", "model_news", "model_pre_train", "model_pre_eval"]


############################################
[data]

start_date = "2016-01-01"

# date before start_data_validation is used for training
start_date_validation = "2019-01-01"
# date after start_data_test is used for testing
start_date_test = "2019-07-01"

end_date = "2019-12-31"



############################################
# parameters for each step in the pipeline
[fin_ingestion]
[fin_ingestion.input]
stock_symbol_file = "stock_symbols.csv"
factor_file = "F-F_Research_Data_5_Factors_2x3_daily"
[fin_ingestion.output]
stock_data_file = "stock_data.csv"
factor_data_file = "ff_5factors_data.csv"


[fin_preprocessing]
[fin_preprocessing.input]
stock_data_file = "stock_data.csv"
factor_data_file = "ff_5factors_data.csv"
[fin_preprocessing.output]
stock_data_cleaned_file = "stock_return.csv"
stock_factor_data_file = "stock_factors_data.csv"


[fin_model]
[fin_model.input]
stock_factor_data = "stock_factors_data.csv"
[fin_model.output]
abnormal_return_file = "abnormal_return.csv"


[news_ingestion]
[news_ingestion.input]
all_news_file = "all-the-news-2-1.csv"
news_head_line = "News_Category_Dataset_v3.json"
all_news_eg1_file = "Example_All_news.csv"
all_news_egk_file = "kExample_All_news.csv"
eda_all_news_file = "EDA_All_news.csv"
all_news_politic_file = "All_news_politics.csv"

# Global parameters for All_news set
columns_to_drop = ['Unnamed: 0', 'author', 'year', 'month', 'day', 'url']
publisher = ["Axios", "Business Insider", "Buzzfeed News", "CNBC", "CNN", "Economist","Fox News", "Gizmodo", "Hyperallergic", "Mashable", "New Republic", "New Yorker", "People", "Politico", "Refinery 29", "Reuters", "TMZ", "TechCrunch", "The Hill", "The New York Times", "The Verge", "Vice", "Vice News", "Vox", "Washington Post", "Wired"]    
subsets_dropna = ['article']
num_words = 100
k = 50


[news_preprocessing]
[news_preprocessing.output]
all_news_cleaned_file = "Preprocessed_All_news.csv"
all_news_politic_cleaned_file = "Preprocessed_All_news_politics.csv"
all_news_eg1_cleaned_file = "Preprocessed_Example_All_news.csv"
news_head_line_cleaned = "Preprocessed_Headline.csv"

[news_model]
[news_model.input]
tfidf_headline = "tfidf_headline.joblib"
tfidf_nmf_headline = "tfidf_nmf_headline.joblib"
word2vec_headline = "word2vec_headline.joblib"
classification_headlines = "classification_headline.joblib"


[predict_model]


[predict_evaluation]



############################################
# automatically generate when run the predic_stock_w_news.py
# for virsion control of files
[date]